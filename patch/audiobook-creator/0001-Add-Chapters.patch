From d7fdc9894d101b27f672b0287ef07f3c0717dbdf Mon Sep 17 00:00:00 2001
From: Raghav <ragha.nv@gmail.com>
Date: Sat, 11 Oct 2025 16:27:17 -0700
Subject: [PATCH] Add chapters

---
 app.py                   | 250 +++++++++-------
 generate_audiobook.py    | 606 +++++++++++++++++++--------------------
 utils/audiobook_utils.py | 188 +++++++++---
 3 files changed, 588 insertions(+), 456 deletions(-)

diff --git a/app.py b/app.py
index 8035285..a442539 100644
--- a/app.py
+++ b/app.py
@@ -19,6 +19,7 @@ along with this program.  If not, see <https://www.gnu.org/licenses/>.
 import gradio as gr
 import os
 import traceback
+import json
 from fastapi import FastAPI
 from book_to_txt import process_book_and_extract_text, save_book
 from identify_characters_and_output_book_to_jsonl import process_book_and_identify_characters
@@ -38,25 +39,44 @@ def validate_book_upload(book_file, book_title):
     """Validate book upload and return a notification"""
     if book_file is None:
         return gr.Warning("Please upload a book file first.")
-    
+
     if not book_title:
         return gr.Warning("Please enter a book title.")
-    
+
     return gr.Info(f"Book '{book_title}' ready for processing.", duration=5)
 
+def validate_knowledge_base(kb_file):
+    """Validate knowledge base JSON file and return a notification"""
+    if kb_file is None:
+        return gr.Info("No knowledge base provided. Character identification will use default method.", duration=5)
+
+    try:
+        with open(kb_file, 'r', encoding='utf-8') as f:
+            kb_data = json.load(f)
+
+        # Basic validation - check if it's a valid JSON structure
+        if not isinstance(kb_data, (dict, list)):
+            return gr.Warning("Knowledge base must be a JSON object or array.")
+
+        return gr.Info(f"Knowledge base loaded successfully! ({len(kb_data) if isinstance(kb_data, list) else len(kb_data.keys())} entries)", duration=5)
+    except json.JSONDecodeError as e:
+        return gr.Warning(f"Invalid JSON format in knowledge base: {str(e)}")
+    except Exception as e:
+        return gr.Warning(f"Error reading knowledge base: {str(e)}")
+
 def text_extraction_wrapper(book_file, text_decoding_option, book_title):
     """Wrapper for text extraction with validation and progress updates"""
     if book_file is None or not book_title:
         yield None
         return gr.Warning("Please upload a book file and enter a title first.")
-    
+
     try:
         last_output = None
         # Pass through all yield values from the original function
         for output in process_book_and_extract_text(book_file, text_decoding_option):
             last_output = output
             yield output  # Yield each progress update
-        
+
         # Final yield with success notification
         yield last_output
         return gr.Info("Text extracted successfully! You can now edit the content.", duration=5)
@@ -76,10 +96,10 @@ def save_book_wrapper(text_content, book_title):
     """Wrapper for saving book with validation"""
     if not text_content:
         return gr.Warning("No text content to save.")
-    
+
     if not book_title:
         return gr.Warning("Please enter a book title before saving.")
-    
+
     try:
         save_book(text_content)
         return gr.Info("üìñ Book saved successfully as 'converted_book.txt'!", duration=10)
@@ -101,7 +121,7 @@ async def identify_characters_wrapper(book_title):
         async for output in process_book_and_identify_characters(book_title):
             last_output = output
             yield output  # Yield each progress update
-        
+
         # Final yield with success notification
         yield gr.Info("Character identification complete! You can now add emotion tags or proceed to audiobook generation.", duration=5)
         yield last_output
@@ -129,7 +149,7 @@ async def add_emotion_tags_wrapper():
         async for output in process_emotion_tags():
             last_output = output
             yield output
-        
+
         # Final yield with success notification
         yield gr.Info("Emotion tags added successfully! You can now generate the audiobook.", duration=5)
         yield last_output
@@ -141,61 +161,62 @@ async def add_emotion_tags_wrapper():
         yield None
         return
 
-async def generate_audiobook_wrapper(voice_type, narrator_gender, output_format, book_file, emotion_tags_processed_state):
+async def generate_audiobook_wrapper(voice_type, narrator_gender, output_format, book_file, emotion_tags_processed_state, kb_file):
     """Wrapper for audiobook generation with validation and progress updates"""
     if book_file is None:
-        yield gr.Warning("Please upload a book file first."), None
-        yield None, None
+        yield gr.Warning("Please upload a book file first."), None, None
+        yield None, None, None
         return
     if not voice_type or not output_format:
-        yield gr.Warning("Please select voice type and output format."), None
-        yield None, None
+        yield gr.Warning("Please select voice type and output format."), None, None
+        yield None, None, None
         return
-    
+
     # Early validation for M4B format
     if output_format == "M4B (Chapters & Cover)":
-        yield gr.Info("Validating book file for M4B audiobook generation..."), None
+        yield gr.Info("Validating book file for M4B audiobook generation..."), None, None
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_file)
-        
+
         if not is_valid:
-            yield gr.Warning(f"‚ùå Book validation failed: {error_message}"), None
-            yield None, None
+            yield gr.Warning(f"‚ùå Book validation failed: {error_message}"), None, None
+            yield None, None, None
             return
-            
-        yield gr.Info(f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"), None
-    
+
+        yield gr.Info(f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"), None, None
+
     # Use session state to determine if emotion tags should be used
     add_emotion_tags = emotion_tags_processed_state
-    
+
     if add_emotion_tags:
-        yield gr.Info("üé≠ Using emotion tags (processed in current session)"), None
+        yield gr.Info("üé≠ Using emotion tags (processed in current session)"), None, None
     else:
-        yield gr.Info("üìñ Using standard narration"), None
-    
+        yield gr.Info("üìñ Using standard narration"), None, None
+
     try:
         last_output = None
         audiobook_path = None
         # Pass through all yield values from the original function
-        async for output in process_audiobook_generation(voice_type, narrator_gender, output_format, book_file, add_emotion_tags):
+        async for output in process_audiobook_generation(voice_type, narrator_gender, output_format, book_file, add_emotion_tags, kb_file):
             last_output = output
-            yield output, None  # Yield each progress update without file path
-        
+            yield output, None, None  # Yield each progress update without file path
+
         # Get the correct file extension based on the output format
         generate_m4b_audiobook_file = True if output_format == "M4B (Chapters & Cover)" else False
         file_extension = "m4b" if generate_m4b_audiobook_file else output_format.lower()
-        
+
         # Set the audiobook file path according to the provided information
         audiobook_path = os.path.join("generated_audiobooks", f"audiobook.{file_extension}")
-        
+        kb_output_path = os.path.join("generated_audiobooks", "knowledge_base.json")
+
         # Final yield with success notification and file path
-        yield gr.Info(f"Audiobook generated successfully in {output_format} format! You can now download it in the Download section. Click on the blue download link next to the file name.", duration=10), None
-        yield last_output, audiobook_path
+        yield gr.Info(f"Audiobook generated successfully in {output_format} format! You can now download it in the Download section. Click on the blue download link next to the file name.", duration=10), None, None
+        yield last_output, audiobook_path, kb_output_path
         return
     except Exception as e:
         print(e)
         traceback.print_exc()
-        yield gr.Warning(f"Error generating audiobook: {str(e)}"), None
-        yield None, None
+        yield gr.Warning(f"Error generating audiobook: {str(e)}"), None, None
+        yield None, None, None
         return
 
 def update_emotion_tags_status_and_state():
@@ -206,80 +227,80 @@ def update_emotion_tags_status_and_state():
 with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     gr.Markdown("# üìñ Audiobook Creator")
     gr.Markdown("Create professional audiobooks from your ebooks in just a few steps.")
-    
+
     # Session state to track if emotion tags were processed
     emotion_tags_processed = gr.State(False)
-    
+
     # Get TTS configuration from environment variables
     current_tts_engine = os.environ.get("TTS_MODEL", "kokoro").lower()
     tts_base_url = os.environ.get("TTS_BASE_URL", "Not configured")
-    
+
     with gr.Row():
         with gr.Column(scale=1):
             gr.Markdown('<div class="step-heading">üìö Step 1: Book Details</div>')
-            
+
             book_title = gr.Textbox(
-                label="Book Title", 
+                label="Book Title",
                 placeholder="Enter the title of your book",
                 info="This will be used for finding the protagonist of the book in the character identification step"
             )
-            
+
             book_input = gr.File(
                 label="Upload Book"
             )
-            
+
             text_decoding_option = gr.Radio(
-                ["textract", "calibre"], 
-                label="Text Extraction Method", 
+                ["textract", "calibre"],
+                label="Text Extraction Method",
                 value="textract",
                 info="Use calibre for better formatted results, wider compatibility for ebook formats. You can try both methods and choose based on the output result."
             )
-            
+
             validate_btn = gr.Button("Validate Book", variant="primary")
 
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">‚úÇÔ∏è Step 2: Extract & Edit Content</div>')
-            
+
             convert_btn = gr.Button("Extract Text", variant="primary")
-            
+
             with gr.Accordion("Editing Tips", open=True):
                 gr.Markdown("""
                 * Remove unwanted sections: Table of Contents, About the Author, Acknowledgements
                 * Fix formatting issues or OCR errors
                 * Check for chapter breaks and paragraph formatting
                 """)
-            
+
             # Navigation buttons for the textbox
             with gr.Row():
                 top_btn = gr.Button("‚Üë Go to Top", size="sm", variant="secondary")
                 bottom_btn = gr.Button("‚Üì Go to Bottom", size="sm", variant="secondary")
-            
+
             text_output = gr.Textbox(
-                label="Edit Book Content", 
+                label="Edit Book Content",
                 placeholder="Extracted text will appear here for editing",
-                interactive=True, 
+                interactive=True,
                 lines=15,
                 elem_id="text_editor"
             )
-            
+
             save_btn = gr.Button("Save Edited Text", variant="primary")
 
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">üß© Step 3: Character Identification (Optional - Requires LLM)</div>')
-            
+
             identify_btn = gr.Button("Identify Characters", variant="primary")
-            
+
             with gr.Accordion("Why Identify Characters?", open=True):
                 gr.Markdown("""
                 * Improves multi-voice narration by assigning different voices to characters
                 * Creates more engaging audiobooks with distinct character voices
                 * Skip this step if you prefer single-voice narration
                 """)
-                
+
             character_output = gr.Textbox(
-                label="Character Identification Progress", 
+                label="Character Identification Progress",
                 placeholder="Character identification progress will be shown here",
                 interactive=False,
                 lines=3
@@ -290,9 +311,9 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     with gr.Row(visible=emotion_tags_visible):
         with gr.Column():
             gr.Markdown('<div class="step-heading">üé≠ Step 3.5: Add Emotion Tags (Optional - Requires LLM)</div>')
-            
+
             emotion_tags_btn = gr.Button("Add Emotion Tags", variant="primary")
-            
+
             with gr.Accordion("What are Emotion Tags?", open=True):
                 gr.Markdown("""
                 **Emotion Tags enhance your audiobook by adding natural expressions:**
@@ -305,12 +326,12 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
                 * **`<groan>`** - For groaning sounds expressing discomfort/frustration
                 * **`<yawn>`** - For yawning or expressions of tiredness
                 * **`<gasp>`** - For gasping sounds of surprise/shock
-                
+
                 These tags are automatically placed based on the text context and work only with **Orpheus TTS**.
                 """)
-                
+
             emotion_tags_output = gr.Textbox(
-                label="Emotion Tags Processing Progress", 
+                label="Emotion Tags Processing Progress",
                 placeholder="Emotion tags processing progress will be shown here",
                 interactive=False,
                 lines=3
@@ -319,39 +340,47 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">üéß Step 4: Generate Audiobook</div>')
-            
+
+            # Knowledge base file input
+            kb_input = gr.File(
+                label="Knowledge Base (Optional)",
+                file_types=[".json", ".txt"]
+            )
+
+            validate_kb_btn = gr.Button("Validate Knowledge Base", variant="secondary", size="sm")
+
             with gr.Row():
                 voice_type = gr.Radio(
-                    ["Single Voice", "Multi-Voice"], 
+                    ["Single Voice", "Multi-Voice"],
                     label="Narration Type",
                     value="Single Voice",
                     info="Multi-Voice requires character identification"
                 )
 
                 narrator_gender = gr.Radio(
-                    ["male", "female"], 
+                    ["male", "female"],
                     label="Choose whether you want the book to be read in a male or female voice",
                     value="female"
                 )
-                
+
                 tts_engine_display = gr.Radio(
-                    ["kokoro", "orpheus"], 
+                    ["kokoro", "orpheus"],
                     label="TTS Engine",
                     value=current_tts_engine,
                     interactive=False,
                     info="Configure TTS engine in .env file. Orpheus supports emotion tags."
                 )
-                
+
                 output_format = gr.Dropdown(
-                    ["M4B (Chapters & Cover)", "AAC", "M4A", "MP3", "WAV", "OPUS", "FLAC", "PCM"], 
+                    ["M4B (Chapters & Cover)", "AAC", "M4A", "MP3", "WAV", "OPUS", "FLAC", "PCM"],
                     label="Output Format",
-                    value="M4B (Chapters & Cover)",
+                    value="M4A",
                     info="M4B supports chapters and cover art"
                 )
-            
+
             # Emotion tags status display (conditional visibility based on TTS engine in .env)
             emotion_tags_visible = current_tts_engine == "orpheus"
-            
+
             with gr.Group(visible=emotion_tags_visible) as emotion_tags_group:
                 emotion_tags_status_display = gr.Radio(
                     choices=["‚úÖ Emotion tags processed - will be used in audiobook", "‚ùå No emotion tags - standard narration will be used"],
@@ -360,56 +389,71 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
                     interactive=False,
                     info="This will update automatically when you process emotion tags in Step 3.5"
                 )
-            
+
             generate_btn = gr.Button("Generate Audiobook", variant="primary")
-            
+
             audio_output = gr.Textbox(
-                label="Generation Progress", 
+                label="Generation Progress",
                 placeholder="Generation progress will be shown here",
                 interactive=False,
                 lines=3
             )
-            
-            # Add a new File component for downloading the audiobook
+
+            # Add a new Group for downloading files
             with gr.Group(visible=False) as download_box:
-                gr.Markdown("### üì• Download Your Audiobook")
+                gr.Markdown("### üì• Download Your Files")
+                
+                # File component for audiobook
                 audiobook_file = gr.File(
                     label="Download Generated Audiobook",
                     interactive=False,
                     type="filepath"
                 )
-    
+                
+                # File component for updated knowledge base
+                knowledge_base_file = gr.File(
+                    label="Download Updated Knowledge Base",
+                    interactive=False,
+                    type="filepath"
+                )
+    # File component for updated knowledge base
+    knowledge_base_file = gr.File(
+        label="Download Updated Knowledge Base",
+        interactive=False,
+        type="filepath"
+    )
+
     # Connections with proper handling of Gradio notifications
     validate_btn.click(
-        validate_book_upload, 
-        inputs=[book_input, book_title], 
+        validate_book_upload,
+        inputs=[book_input, book_title],
         outputs=[]
     )
-    
+
     convert_btn.click(
-        text_extraction_wrapper, 
-        inputs=[book_input, text_decoding_option, book_title], 
+        text_extraction_wrapper,
+        inputs=[book_input, text_decoding_option, book_title],
         outputs=[text_output],
         queue=True
     )
-    
+
     save_btn.click(
-        save_book_wrapper, 
-        inputs=[text_output, book_title], 
+        save_book_wrapper,
+        inputs=[text_output, book_title],
         outputs=[],
         queue=True
     )
-    
+
     identify_btn.click(
-        identify_characters_wrapper, 
-        inputs=[book_title], 
+        identify_characters_wrapper,
+        inputs=[book_title],
         outputs=[character_output],
         queue=True
     )
-    
+
     emotion_tags_btn.click(
-        add_emotion_tags_wrapper, 
-        inputs=[], 
+        add_emotion_tags_wrapper,
+        inputs=[],
         outputs=[emotion_tags_output],
         queue=True
     ).then(
@@ -418,20 +462,26 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
         inputs=[],
         outputs=[emotion_tags_status_display, emotion_tags_processed]
     )
-    
+
+    validate_kb_btn.click(
+        validate_knowledge_base,
+        inputs=[kb_input],
+        outputs=[]
+    )
+
     # Update the generate_audiobook_wrapper to output both progress text and file path
     generate_btn.click(
-        generate_audiobook_wrapper, 
-        inputs=[voice_type, narrator_gender, output_format, book_input, emotion_tags_processed], 
-        outputs=[audio_output, audiobook_file],
+        generate_audiobook_wrapper,
+        inputs=[voice_type, narrator_gender, output_format, book_input, emotion_tags_processed, kb_input],
+        outputs=[audio_output, audiobook_file, knowledge_base_file],
         queue=True
     ).then(
         # Make the download box visible after generation completes successfully
-        lambda x: gr.update(visible=True) if x is not None else gr.update(visible=False),
-        inputs=[audiobook_file],
+        lambda audio_path, kb_path: gr.update(visible=True) if audio_path is not None and kb_path is not None else gr.update(visible=False),
+        inputs=[audiobook_file, knowledge_base_file],
         outputs=[download_box]
     )
-    
+
     # Navigation button functionality for textbox scrolling
     top_btn.click(
         None,
@@ -446,7 +496,7 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
         }
         """
     )
-    
+
     bottom_btn.click(
         None,
         inputs=[],
@@ -465,4 +515,4 @@ app = gr.mount_gradio_app(app, gradio_app, path="/")  # Mount Gradio at root
 
 if __name__ == "__main__":
     import uvicorn
-    uvicorn.run(app, host="0.0.0.0", port=7860)
\ No newline at end of file
+    uvicorn.run(app, host="0.0.0.0", port=7860)
diff --git a/generate_audiobook.py b/generate_audiobook.py
index 1711db2..ea0c87b 100755
--- a/generate_audiobook.py
+++ b/generate_audiobook.py
@@ -16,6 +16,7 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import unicodedata
 import shutil
 from openai import OpenAI, AsyncOpenAI
 from tqdm import tqdm
@@ -56,39 +57,45 @@ def sanitize_filename(text):
     text = text.replace("'", '').replace('"', '').replace('/', ' ').replace('.', ' ')
     text = text.replace(':', '').replace('?', '').replace('\\', '').replace('|', '')
     text = text.replace('*', '').replace('<', '').replace('>', '').replace('&', 'and')
-    
-    # Normalize whitespace and trim
-    text = ' '.join(text.split())
-    
+    text = text.replace('!', '').replace('@', '').replace('#', '').replace('$', '').replace('%', '')
+    text = text.replace('^', '').replace('(', '').replace(')', '').replace('[', '').replace(']', '')
+    text = text.replace('{', '').replace('}', '').replace('+', '').replace('=', '')
+    text = text.replace(',', '_').replace(' ', '_')
+
+    # Normalize non-ASCII to closest ASCII equivalent
+    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')
+
+    # Collapse multiple underscores
+    text = re.sub(r'_+', '_', text).strip('_')
     return text
 
 def is_only_punctuation(text):
     """
     Check if a line contains only punctuation marks without any actual words.
     This helps avoid TTS errors when encountering lines with just punctuation.
-    
+
     Args:
         text (str): The text line to check
-        
+
     Returns:
         bool: True if the line contains only punctuation, False otherwise
     """
     # Remove all whitespace
     cleaned_text = text.strip()
-    
+
     # If empty after stripping, it's not useful for TTS
     if not cleaned_text:
         return True
-    
+
     # Import string for standard punctuation
     import string
-    
+
     # Extended punctuation set including common Unicode punctuation in books
     extended_punctuation = string.punctuation + '‚Äî‚Äì""''‚Ä¶‚Äö‚Äû‚Äπ‚Ä∫¬´¬ª‚Ä∞‚Ä±'
-    
+
     # Remove all punctuation marks (both ASCII and extended Unicode)
     text_without_punct = ''.join(char for char in cleaned_text if char not in extended_punctuation)
-    
+
     # If nothing remains after removing punctuation, it's only punctuation
     return len(text_without_punct.strip()) == 0
 
@@ -130,13 +137,13 @@ def check_if_chapter_heading(text):
         except ValueError:
             return False  # Invalid number format
     return False  # No match
-    
+
 def find_voice_for_gender_score(character: str, character_gender_map, engine_name: str, narrator_gender: str):
     """
     Finds the appropriate voice for a character based on their gender score using the new voice mapping system.
 
     This function takes in the name of a character, a dictionary mapping character names to their gender scores,
-    the TTS engine name, and the narrator gender preference. It returns the voice identifier that matches 
+    the TTS engine name, and the narrator gender preference. It returns the voice identifier that matches
     the character's gender score within the appropriate score map (male_score_map or female_score_map).
 
     Args:
@@ -157,7 +164,7 @@ def find_voice_for_gender_score(character: str, character_gender_map, engine_nam
     if "scores" in character_gender_map and character.lower() in character_gender_map["scores"]:
         character_info = character_gender_map["scores"][character.lower()]
         character_gender_score = character_info["gender_score"]
-        
+
         return get_voice_for_character_score(engine_name, narrator_gender, character_gender_score)
     else:
         # Fallback for unknown characters - use score 5 (neutral)
@@ -166,16 +173,16 @@ def find_voice_for_gender_score(character: str, character_gender_map, engine_nam
 def validate_book_for_m4b_generation(book_path):
     """
     Validates that the book file is suitable for M4B audiobook generation.
-    
+
     This function performs early validation to catch issues before audio generation:
     - Checks if the book file path is safe and accessible
     - Verifies that ebook-meta command is available
     - Tests metadata extraction from the book
     - Ensures cover image can be extracted
-    
+
     Args:
         book_path (str): Path to the book file
-        
+
     Returns:
         tuple: (is_valid, error_message, metadata)
             - is_valid (bool): True if validation passed
@@ -186,20 +193,20 @@ def validate_book_for_m4b_generation(book_path):
         # Validate file path safety and existence
         if not validate_file_path(book_path):
             return False, f"Invalid or inaccessible book file: {book_path}. Please check the file path and permissions.", None
-        
+
         # Test metadata extraction (this also validates ebook-meta availability)
         metadata = get_ebook_metadata_with_cover(book_path)
-        
+
         # Check if we got meaningful metadata
         if not metadata or len(metadata) == 0:
             return False, f"No metadata could be extracted from the book file: {book_path}. Please ensure it's a valid ebook format.", None
-            
+
         # Check if cover extraction worked (cover.jpg should exist after get_ebook_metadata_with_cover)
         if not validate_file_path("cover.jpg"):
             return False, f"Could not extract cover image from the book file: {book_path}. The book may not contain a cover image.", None
-            
+
         return True, None, metadata
-        
+
     except ValueError as e:
         return False, f"Book file validation error: {str(e)}", None
     except RuntimeError as e:
@@ -207,6 +214,151 @@ def validate_book_for_m4b_generation(book_path):
     except Exception as e:
         return False, f"Unexpected error during book validation: {str(e)}", None
 
+async def process_audio_results_and_generate_audiobook(
+    results_all,
+    task_to_index,
+    tasks,
+    progress_counter,
+    total_lines,
+    temp_line_audio_dir,
+    temp_audio_dir,
+    chapter_files,
+    output_format,
+    generate_m4b_audiobook_file,
+    book_path,
+    progress_bar
+):
+    """
+    Common function to process audio results and generate the final audiobook.
+    Extracted from both generate_audio_with_single_voice and generate_audio_with_multiple_voices.
+    """
+    # Initialize chapter tracking variables
+    chapter_index = 1
+    current_chapter_audio = "Introduction.wav"
+    chapter_line_map = {}
+
+    # Process tasks with progress updates
+    last_reported = -1
+    latest_chapter_name = None
+    while tasks:
+        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
+
+        # Store results as tasks complete
+        for completed_task in done:
+            idx = task_to_index[completed_task]
+            result = completed_task.result()
+            results_all[idx] = result
+
+            if result and result["is_chapter_heading"]:
+                latest_chapter_name = result['line']
+
+        tasks = list(pending)
+
+        # Only yield if the counter has changed
+        if progress_counter > last_reported:
+            last_reported = progress_counter
+            percent = (progress_counter / total_lines) * 100
+            yield f"Generating audiobook. Chapter: {latest_chapter_name or 'N/A'}. Progress: {percent:.1f}%"
+
+    # All tasks have completed at this point and results_all is populated
+    results = [r for r in results_all if r is not None]  # Filter out empty lines
+
+    progress_bar.close()
+
+    # Filter out empty lines (same as in your original code)
+    results = [r for r in results_all if r is not None]
+
+    yield "Completed generating audio for all lines"
+
+    # Second pass: Organize by chapters
+    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
+    yield "Organizing lines into chapters"
+
+    chapter_line_text_map = {}
+    for result in sorted(results, key=lambda x: x["index"]):
+        # Check if this is a chapter heading
+        if result["is_chapter_heading"]:
+            chapter_index += 1
+            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
+
+        if current_chapter_audio not in chapter_files:
+            chapter_files.append(current_chapter_audio)
+            chapter_line_map[current_chapter_audio] = []
+
+        # Add this line index to the chapter
+        chapter_line_map[current_chapter_audio].append(result["index"])
+        chapter_line_text_map[result["index"]] = result["line"]
+        chapter_organization_bar.update(1)
+
+    chapter_organization_bar.close()
+    yield f"Organized {len(results)} lines into {len(chapter_files)} chapters"
+
+    # Third pass: Concatenate audio files for each chapter in order
+    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
+
+    SILENCE_DURATION_MS = 1000
+    line_timings = []
+    for chapter_file in chapter_files:
+        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
+        assemble_chapter_with_ffmpeg(
+            chapter_file,
+            chapter_line_map[chapter_file],
+            temp_line_audio_dir,
+            temp_audio_dir,
+            chapter_line_text_map,
+            line_timings,
+            SILENCE_DURATION_MS
+        )
+
+        chapter_assembly_bar.update(1)
+        yield f"Assembled chapter: {chapter_file}"
+
+    chapter_assembly_bar.close()
+    yield "Completed assembling all chapters"
+
+    # Post-processing steps
+    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
+
+    # Add silence to each chapter file using FFmpeg
+    for chapter_file in chapter_files:
+        chapter_path = os.path.join(temp_audio_dir, chapter_file)
+
+        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
+        add_silence_to_chapter_with_ffmpeg(chapter_path, SILENCE_DURATION_MS)  # 1 second silence
+
+        post_processing_bar.update(1)
+        yield f"Added silence to chapter: {chapter_file}"
+
+    m4a_chapter_files = []
+
+    # Convert all chapter files to M4A format
+    for chapter_file in chapter_files:
+        chapter_name = chapter_file.split('.')[0]
+        m4a_chapter_files.append(f"{chapter_name}.m4a")
+        # Convert WAV to M4A for better compatibility with timestamps and metadata
+        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
+        post_processing_bar.update(1)
+        yield f"Converted chapter to M4A: {chapter_name}"
+
+    post_processing_bar.close()
+
+    # Clean up temp line audio files
+    yield "Cleaning up temporary files"
+    shutil.rmtree(temp_line_audio_dir)
+    yield "Temporary files cleanup complete"
+
+    if generate_m4b_audiobook_file:
+        # Merge all chapter files into a final m4b audiobook
+        yield "Creating M4B audiobook file..."
+        merge_chapters_to_m4b(book_path, m4a_chapter_files)
+        yield "M4B audiobook created successfully"
+    else:
+        # Merge all chapter files into a standard M4A audiobook
+        yield "Creating final audiobook..."
+        merge_chapters_to_standard_audio_file(m4a_chapter_files, line_timings)
+        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
+        yield f"Audiobook in {output_format} format created successfully"
+
 async def generate_audio_with_single_voice(output_format, narrator_gender, generate_m4b_audiobook_file=False, book_path="", add_emotion_tags=False):
     # Read the text from the file
     """
@@ -228,15 +380,15 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
         str: Progress updates as the audiobook generation progresses through loading text, generating audio,
              organizing by chapters, assembling chapters, and post-processing steps.
     """
-    
+
     # Early validation for M4B generation
     if generate_m4b_audiobook_file:
         yield "Validating book file for M4B audiobook generation..."
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             raise ValueError(f"‚ùå Book validation failed: {error_message}")
-            
+
         yield f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"
 
     # Check if emotion tags should be used and if they have been pre-applied
@@ -247,20 +399,20 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
     else:
         with open("converted_book.txt", "r", encoding='utf-8') as f:
             text = f.read()
-        
+
         # Apply text preprocessing for Orpheus TTS to prevent repetition issues
         if TTS_MODEL.lower() == "orpheus":
             text = preprocess_text_for_tts(text)
             yield "Applied text preprocessing for Orpheus TTS"
-    
+
     lines = text.split("\n")
-    
+
     # Filter out empty lines
     lines = [line.strip() for line in lines if line.strip()]
-    
+
     # Set the voices to be used - now using the new voice mapping system
     narrator_voice, dialogue_voice = get_narrator_and_dialogue_voices(
-        engine_name=TTS_MODEL, 
+        engine_name=TTS_MODEL,
         narrator_gender=narrator_gender
     )
 
@@ -272,26 +424,21 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
 
     os.makedirs(temp_audio_dir, exist_ok=True)
     os.makedirs(temp_line_audio_dir, exist_ok=True)
-    
+
     # Batch processing parameters
     semaphore = asyncio.Semaphore(TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE)
-    
+
     # Initial setup for chapters
-    chapter_index = 1
-    current_chapter_audio = f"Introduction.wav"
     chapter_files = []
-    
+
     # First pass: Generate audio for each line independently
     total_size = len(lines)
 
     progress_counter = 0
-    
+
     # For tracking progress with tqdm in an async context
     progress_bar = tqdm(total=total_size, unit="line", desc="Audio Generation Progress")
-    
-    # Maps chapters to their line indices
-    chapter_line_map = {}
-    
+
     async def process_single_line(line_index, line):
         async with semaphore:
             nonlocal progress_counter
@@ -300,13 +447,13 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
                 progress_bar.update(1)
                 progress_counter += 1
                 return None
-                
+
             # Split the line into annotated parts
             annotated_parts = split_and_annotate_text(line)
-            
+
             # Create combined audio using PyDub for seamless concatenation
             combined_audio = AudioSegment.empty()
-            
+
             for part in annotated_parts:
                 text_to_speak = part["text"].strip()
 
@@ -314,54 +461,54 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
                     continue
 
                 voice_to_speak_in = narrator_voice if part["type"] == "narration" else dialogue_voice
-                
+
                 # Create temporary file for this part
                 temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                 temp_path = temp_file.name
                 temp_file.close()
-                
+
                 try:
                     # Generate audio for the part using retry mechanism
                     audio_buffer = await generate_audio_with_retry(
-                        async_openai_client, 
+                        async_openai_client,
                         TTS_MODEL,
-                        text_to_speak, 
+                        text_to_speak,
                         voice_to_speak_in
                     )
-                    
+
                     # Write part audio to temp file
                     with open(temp_path, "wb") as temp_wav:
                         temp_wav.write(audio_buffer)
-                    
+
                     # Load as AudioSegment and add to combined audio
                     part_segment = AudioSegment.from_wav(temp_path)
                     combined_audio += part_segment
-                    
+
                 except Exception as e:
                     # Log the error for debugging
                     print(f"Warning: Failed to generate audio for text: '{text_to_speak[:50]}...' - Error: {str(e)}")
                     # Skip this part and continue with next part
-                    
+
                 finally:
                     # Always clean up temp file
                     if os.path.exists(temp_path):
                         os.unlink(temp_path)
-            
+
             # Check if we have any audio content before exporting
             if len(combined_audio) == 0:
                 # If no audio was generated for this line, skip it entirely
                 progress_bar.update(1)
                 progress_counter += 1
                 return None
-            
+
             # Write this line's audio to a temporary file
             line_audio_path = os.path.join(temp_line_audio_dir, f"line_{line_index:06d}.wav")
             combined_audio.export(line_audio_path, format="wav")
-            
+
             # Update progress bar
             progress_bar.update(1)
             progress_counter += 1
-            
+
             return {
                 "index": line_index,
                 "is_chapter_heading": check_if_chapter_heading(line),
@@ -375,129 +522,38 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
         task = asyncio.create_task(process_single_line(i, line))
         tasks.append(task)
         task_to_index[task] = i
-    
+
     # Initialize results_all list
     results_all = [None] * len(lines)
-    
-    # Process tasks with progress updates
-    last_reported = -1
-    while tasks:
-        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
-        
-        # Store results as tasks complete
-        for completed_task in done:
-            idx = task_to_index[completed_task]
-            results_all[idx] = completed_task.result()
-        
-        tasks = list(pending)
-        
-        # Only yield if the counter has changed
-        if progress_counter > last_reported:
-            last_reported = progress_counter
-            percent = (progress_counter / total_size) * 100
-            yield f"Generating audiobook. Progress: {percent:.1f}%"
-    
-    # All tasks have completed at this point and results_all is populated
-    results = [r for r in results_all if r is not None]  # Filter out empty lines
-    
-    progress_bar.close()
-    
-    # Filter out empty lines (same as in your original code)
-    results = [r for r in results_all if r is not None]
-    
-    yield "Completed generating audio for all lines"
-
-    # Second pass: Organize by chapters
-    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
-    
-    for result in sorted(results, key=lambda x: x["index"]):
-        # Check if this is a chapter heading
-        if result["is_chapter_heading"]:
-            chapter_index += 1
-            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
-            
-        if current_chapter_audio not in chapter_files:
-            chapter_files.append(current_chapter_audio)
-            chapter_line_map[current_chapter_audio] = []
-            
-        # Add this line index to the chapter
-        chapter_line_map[current_chapter_audio].append(result["index"])
-        chapter_organization_bar.update(1)
-    
-    chapter_organization_bar.close()
-    yield "Organizing audio by chapters complete"
-    
-    # Third pass: Concatenate audio files for each chapter in order
-    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
-    
-    for chapter_file in chapter_files:
-        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
-        assemble_chapter_with_ffmpeg(
-            chapter_file, 
-            chapter_line_map[chapter_file], 
-            temp_line_audio_dir, 
-            temp_audio_dir
-        )
-        
-        chapter_assembly_bar.update(1)
-        yield f"Assembled chapter: {chapter_file}"
-    
-    chapter_assembly_bar.close()
-    yield "Completed assembling all chapters"
-    
-    # Post-processing steps
-    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
-    
-    # Add silence to each chapter file using FFmpeg
-    for chapter_file in chapter_files:
-        chapter_path = os.path.join(temp_audio_dir, chapter_file)
-        
-        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
-        add_silence_to_chapter_with_ffmpeg(chapter_path, 1000)  # 1 second silence
-        
-        post_processing_bar.update(1)
-        yield f"Added silence to chapter: {chapter_file}"
-
-    m4a_chapter_files = []
-
-    # Convert all chapter files to M4A format
-    for chapter_file in chapter_files:
-        chapter_name = chapter_file.split('.')[0]
-        m4a_chapter_files.append(f"{chapter_name}.m4a")
-        # Convert WAV to M4A for better compatibility with timestamps and metadata
-        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
-        post_processing_bar.update(1)
-        yield f"Converted chapter to M4A: {chapter_name}"
-    
-    post_processing_bar.close()
-    
-    # Clean up temp line audio files
-    shutil.rmtree(temp_line_audio_dir)
-    yield "Cleaned up temporary files"
 
-    if generate_m4b_audiobook_file:
-        # Merge all chapter files into a final m4b audiobook
-        yield "Creating M4B audiobook file..."
-        merge_chapters_to_m4b(book_path, m4a_chapter_files)
-        yield "M4B audiobook created successfully"
-    else:
-        # Merge all chapter files into a standard M4A audiobook
-        yield "Creating final audiobook..."
-        merge_chapters_to_standard_audio_file(m4a_chapter_files)
-        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
-        yield f"Audiobook in {output_format} format created successfully"
+    # Call common processing function
+    async for progress_msg in process_audio_results_and_generate_audiobook(
+        results_all,
+        task_to_index,
+        tasks,
+        progress_counter,
+        total_size,
+        temp_line_audio_dir,
+        temp_audio_dir,
+        chapter_files,
+        output_format,
+        generate_m4b_audiobook_file,
+        book_path,
+        progress_bar
+    ):
+        yield progress_msg
 
 def apply_emotion_tags_to_multi_voice_data(json_data_array):
     """
     Dynamically apply pre-processed emotion tags to multi-voice JSONL data.
-    
+
     This function reads emotion-enhanced text from tag_added_lines_chunks.txt
     and applies it to the speaker-attributed JSONL data in memory, preserving
     speaker attributions while using the enhanced text content.
-    
+
     Args:
         json_data_array (list): Original speaker-attributed JSONL data
-        
+
     Returns:
         tuple: (success, json_data_array, message)
             - success (bool): True if emotion tags were successfully applied
@@ -506,7 +562,7 @@ def apply_emotion_tags_to_multi_voice_data(json_data_array):
     """
     if not os.path.exists("tag_added_lines_chunks.txt"):
         return False, json_data_array, "No pre-processed emotion tags found"
-    
+
     try:
         # Read the enhanced lines from tag_added_lines_chunks.txt
         with open("tag_added_lines_chunks.txt", "r", encoding='utf-8') as f:
@@ -519,7 +575,7 @@ def apply_emotion_tags_to_multi_voice_data(json_data_array):
             return True, json_data_array, "Successfully applied pre-processed emotion tags"
         else:
             return False, json_data_array, f"Line count mismatch: {len(enhanced_lines)} enhanced lines vs {len(json_data_array)} speaker-attributed lines"
-            
+
     except Exception as e:
         return False, json_data_array, f"Error applying emotion tags: {str(e)}"
 
@@ -546,17 +602,17 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     :param book_path: The path to the book file (required for generating an M4B audiobook file)
     :param add_emotion_tags: Whether to use pre-applied emotion tags in the audiobook. Defaults to False.
     """
-    
+
     # Early validation for M4B generation
     if generate_m4b_audiobook_file:
         yield "Validating book file for M4B audiobook generation..."
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             raise ValueError(f"‚ùå Book validation failed: {error_message}")
-            
+
         yield f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"
-    
+
     file_path = 'speaker_attributed_book.jsonl'
     json_data_array = []
 
@@ -587,7 +643,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
             '<yawn>' in item.get('line', '') or '<gasp>' in item.get('line', '')
             for item in json_data_array
         )
-        
+
         if has_emotion_tags:
             yield "Removing existing emotion tags from JSONL data as per user preference"
             import re
@@ -596,7 +652,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
                     # Remove emotion tags from the line
                     line_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*', '', item["line"])
                     item["line"] = line_without_tags
-    
+
     # Apply text preprocessing for Orpheus TTS to prevent repetition issues
     if TTS_MODEL.lower() == "orpheus":
         for item in json_data_array:
@@ -610,7 +666,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     # Get narrator voice using the new voice mapping system
     narrator_voice = find_voice_for_gender_score("narrator", character_gender_map, TTS_MODEL, narrator_gender)
     yield "Loaded voice mappings and selected narrator voice"
-    
+
     # Setup directories
     temp_audio_dir = "temp_audio"
     temp_line_audio_dir = os.path.join(temp_audio_dir, "line_segments")
@@ -620,21 +676,17 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     os.makedirs(temp_audio_dir, exist_ok=True)
     os.makedirs(temp_line_audio_dir, exist_ok=True)
     yield "Set up temporary directories for audio processing"
-    
+
     # Batch processing parameters
     semaphore = asyncio.Semaphore(TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE)
-    
+
     # Initial setup for chapters
-    chapter_index = 1
-    current_chapter_audio = f"Introduction.wav"
     chapter_files = []
-    
+
     # First pass: Generate audio for each line independently
     # and track chapter organization
-    chapter_line_map = {}  # Maps chapters to their line indices
-
     progress_counter = 0
-    
+
     # For tracking progress with tqdm in an async context
     total_lines = len(json_data_array)
     progress_bar = tqdm(total=total_lines, unit="line", desc="Audio Generation Progress")
@@ -654,13 +706,13 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
 
             speaker = doc["speaker"]
             speaker_voice = find_voice_for_gender_score(speaker, character_gender_map, TTS_MODEL, narrator_gender)
-            
+
             # Split the line into annotated parts
             annotated_parts = split_and_annotate_text(line)
-            
+
             # Create combined audio using PyDub for seamless concatenation
             combined_audio = AudioSegment.empty()
-            
+
             for part in annotated_parts:
                 text_to_speak = part["text"].strip()
 
@@ -668,60 +720,60 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
                     continue
 
                 voice_to_speak_in = narrator_voice if part["type"] == "narration" else speaker_voice
-                
+
                 # Create temporary file for this part
                 temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                 temp_path = temp_file.name
                 temp_file.close()
-                
+
                 try:
                     # Generate audio for the part using retry mechanism
                     audio_buffer = await generate_audio_with_retry(
-                        async_openai_client, 
+                        async_openai_client,
                         TTS_MODEL,
-                        text_to_speak, 
+                        text_to_speak,
                         voice_to_speak_in
                     )
-                    
+
                     # Write part audio to temp file
                     with open(temp_path, "wb") as temp_wav:
                         temp_wav.write(audio_buffer)
-                    
+
                     # Load as AudioSegment and add to combined audio
                     part_segment = AudioSegment.from_wav(temp_path)
                     combined_audio += part_segment
-                    
+
                 except Exception as e:
                     # Log the error for debugging
                     print(f"Warning: Failed to generate audio for text: '{text_to_speak[:50]}...' - Error: {str(e)}")
                     # Skip this part and continue with next part
-                    
+
                 finally:
                     # Always clean up temp file
                     if os.path.exists(temp_path):
                         os.unlink(temp_path)
-            
+
             # Check if we have any audio content before exporting
             if len(combined_audio) == 0:
                 # If no audio was generated for this line, skip it entirely
                 progress_bar.update(1)
                 progress_counter += 1
                 return None
-            
+
             # Write this line's audio to a temporary file
             line_audio_path = os.path.join(temp_line_audio_dir, f"line_{line_index:06d}.wav")
             combined_audio.export(line_audio_path, format="wav")
-            
+
             # Update progress bar
             progress_bar.update(1)
             progress_counter += 1
-            
+
             return {
                 "index": line_index,
                 "is_chapter_heading": check_if_chapter_heading(line),
                 "line": line
             }
-    
+
     # Create tasks and store them with their index for result collection
     tasks = []
     task_to_index = {}
@@ -729,126 +781,44 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
         task = asyncio.create_task(process_single_line(i, doc))
         tasks.append(task)
         task_to_index[task] = i
-    
+
     # Initialize results_all list
     results_all = [None] * len(json_data_array)
-    
-    # Process tasks with progress updates
-    last_reported = -1
-    while tasks:
-        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
-        
-        # Store results as tasks complete
-        for completed_task in done:
-            idx = task_to_index[completed_task]
-            results_all[idx] = completed_task.result()
-        
-        tasks = list(pending)
-        
-        # Only yield if the counter has changed
-        if progress_counter > last_reported:
-            last_reported = progress_counter
-            percent = (progress_counter / total_lines) * 100
-            yield f"Generating audiobook. Progress: {percent:.1f}%"
-    
-    # All tasks have completed at this point and results_all is populated
-    results = [r for r in results_all if r is not None]  # Filter out empty lines
-    
-    progress_bar.close()
-    
-    # Filter out empty lines (same as in your original code)
-    results = [r for r in results_all if r is not None]
-    
-    yield "Completed generating audio for all lines"
-    
-    # Second pass: Organize by chapters
-    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
-    yield "Organizing lines into chapters"
-    
-    for result in sorted(results, key=lambda x: x["index"]):
-        # Check if this is a chapter heading
-        if result["is_chapter_heading"]:
-            chapter_index += 1
-            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
-            
-        if current_chapter_audio not in chapter_files:
-            chapter_files.append(current_chapter_audio)
-            chapter_line_map[current_chapter_audio] = []
-            
-        # Add this line index to the chapter
-        chapter_line_map[current_chapter_audio].append(result["index"])
-        chapter_organization_bar.update(1)
-    
-    chapter_organization_bar.close()
-    yield f"Organized {len(results)} lines into {len(chapter_files)} chapters"
-    
-    # Third pass: Concatenate audio files for each chapter in order
-    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
-    
-    for chapter_file in chapter_files:
-        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
-        assemble_chapter_with_ffmpeg(
-            chapter_file, 
-            chapter_line_map[chapter_file], 
-            temp_line_audio_dir, 
-            temp_audio_dir
-        )
-        
-        chapter_assembly_bar.update(1)
-        yield f"Assembled chapter: {chapter_file}"
-    
-    chapter_assembly_bar.close()
-    yield "Completed assembling all chapters"
-    
-    # Post-processing steps
-    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
-    
-    # Add silence to each chapter file using FFmpeg
-    for chapter_file in chapter_files:
-        chapter_path = os.path.join(temp_audio_dir, chapter_file)
-        
-        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
-        add_silence_to_chapter_with_ffmpeg(chapter_path, 1000)  # 1 second silence
-        
-        post_processing_bar.update(1)
-        yield f"Added silence to chapter: {chapter_file}"
-
-    m4a_chapter_files = []
 
-    # Convert all chapter files to M4A format
-    for chapter_file in chapter_files:
-        chapter_name = chapter_file.split('.')[0]
-        m4a_chapter_files.append(f"{chapter_name}.m4a")
-        # Convert WAV to M4A for better compatibility with timestamps and metadata
-        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
-        post_processing_bar.update(1)
-        yield f"Converted chapter to M4A: {chapter_name}"
-    
-    post_processing_bar.close()
-    
-    # Clean up temp line audio files
-    yield "Cleaning up temporary files"
-    shutil.rmtree(temp_line_audio_dir)
-    yield "Temporary files cleanup complete"
-
-    if generate_m4b_audiobook_file:
-        # Merge all chapter files into a final m4b audiobook
-        yield "Creating M4B audiobook file..."
-        merge_chapters_to_m4b(book_path, m4a_chapter_files)
-        yield "M4B audiobook created successfully"
-    else:
-        # Merge all chapter files into a standard M4A audiobook
-        yield "Creating final audiobook..."
-        merge_chapters_to_standard_audio_file(m4a_chapter_files)
-        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
-        yield f"Audiobook in {output_format} format created successfully"
-
-async def process_audiobook_generation(voice_option, narrator_gender, output_format, book_path, add_emotion_tags=False):
+    # Call common processing function
+    async for progress_msg in process_audio_results_and_generate_audiobook(
+        results_all,
+        task_to_index,
+        tasks,
+        progress_counter,
+        total_lines,
+        temp_line_audio_dir,
+        temp_audio_dir,
+        chapter_files,
+        output_format,
+        generate_m4b_audiobook_file,
+        book_path,
+        progress_bar
+    ):
+        yield progress_msg
+
+async def process_audiobook_generation(voice_option, narrator_gender, output_format, book_path, add_emotion_tags=False, kb_file=None):
     is_audio_generator_api_up, message = await check_if_audio_generator_api_is_up(async_openai_client)
 
     if not is_audio_generator_api_up:
         raise Exception(message)
 
+    # Load knowledge base if provided
+    kb_data = None
+    if kb_file:
+        try:
+            import json
+            with open(kb_file, 'r', encoding='utf-8') as f:
+                kb_data = json.load(f)
+            yield f"üìö Knowledge base loaded with {len(kb_data) if isinstance(kb_data, list) else len(kb_data.keys())} entries"
+        except Exception as e:
+            yield f"‚ö†Ô∏è Warning: Could not load knowledge base: {str(e)}"
+
     generate_m4b_audiobook_file = False
 
     if output_format == "M4B (Chapters & Cover)":
@@ -867,7 +837,7 @@ async def process_audiobook_generation(voice_option, narrator_gender, output_for
                 yield line
 
         yield f"\nüéß Audiobook is generated ! You can now download it in the Download section below. Click on the blue download link next to the file name."
-        
+
     except ValueError as e:
         # Handle validation errors specifically
         error_msg = str(e)
@@ -910,7 +880,7 @@ async def main():
         if not is_calibre_installed:
             print("‚ö†Ô∏è Calibre is not installed. Please install it first and make sure **calibre** and **ebook-meta** commands are available in your PATH.")
             return
-        
+
         is_ffmpeg_installed = check_if_ffmpeg_is_installed()
 
         if not is_ffmpeg_installed:
@@ -929,11 +899,11 @@ async def main():
             print(f"üìÇ Using book file: **{book_path}**")
 
         print("‚úÖ Book path set. Proceeding...\n")
-        
+
         # Early validation of the book file for M4B generation
         print("üîç Validating book file for M4B audiobook generation...")
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             print(f"‚ùå **Book validation failed**: {error_message}")
             print("\nüí° **Troubleshooting Tips:**")
@@ -943,7 +913,7 @@ async def main():
             print("   ‚Ä¢ Make sure the book file is not corrupted")
             print("   ‚Ä¢ Ensure the book file contains extractable metadata and cover image")
             return
-            
+
         print(f"‚úÖ **Book validation successful!**")
         print(f"   ‚Ä¢ Title: {metadata.get('Title', 'Unknown')}")
         print(f"   ‚Ä¢ Author: {metadata.get('Author(s)', 'Unknown')}")
@@ -959,7 +929,7 @@ async def main():
         if(output_format not in ["aac", "m4a", "mp3", "wav", "opus", "flac", "pcm"]):
             print("\n‚ö†Ô∏è Invalid output format! Please choose from the give options")
             return
-        
+
     # Prompt user for narrator's gender selection
     print("\nüéôÔ∏è **Audiobook Narrator Voice Selection**")
     narrator_gender = input("üîπ Enter **male** if you want the book to be read in a male voice or **female** if you want the book to be read in a female voice: ").strip()
@@ -975,7 +945,7 @@ async def main():
         print("üîπ Emotion tags add natural expressions like laughter, sighs, gasps to your audiobook")
         print("üîπ Available tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>")
         emotion_tags_option = input("üîπ Do you want to use emotion tags in the audiobook? Enter **yes** or **no**: ").strip().lower()
-        
+
         if emotion_tags_option in ["yes", "y", "true", "1"]:
             add_emotion_tags = True
             print("‚úÖ Emotion tags will be used in the audiobook!")
diff --git a/utils/audiobook_utils.py b/utils/audiobook_utils.py
index df8ac0b..5c935e2 100644
--- a/utils/audiobook_utils.py
+++ b/utils/audiobook_utils.py
@@ -16,6 +16,8 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import tempfile
+import shutil
 import subprocess
 import re
 import os
@@ -655,53 +657,150 @@ def add_silence_to_audio_file_by_reencoding_using_ffmpeg(temp_dir, input_file_na
     except OSError as e:
         raise RuntimeError(f"Failed to rename file: {e}")
 
-def merge_chapters_to_standard_audio_file(chapter_files):
+def generate_line_timings_json(line_timings):
     """
-    Uses ffmpeg to merge all chapter files into a standard M4A audio file).
+    Convert line timing data to a compact JSON string suitable for embedding
+    in FFmpeg metadata (comment field).
 
-    This function takes a list of chapter files and an output format as input, and generates a standard M4A audio file.
+    Escapes special characters per FFMETADATA spec:
+        '=', ';', '#', '\\', and newline.
+    """
+    if not line_timings:
+        return None
+
+    def escape_ffmetadata_chars(text: str) -> str:
+        text = text.replace("\\", "\\\\")
+        text = text.replace("=", "\\=")
+        text = text.replace(";", "\\;")
+        text = text.replace("#", "\\#")
+        return text
+
+    line_data = [
+        {
+            "startTime": round(line["start"], 3),
+            "endTime": round(line["end"], 3),
+            "text": line["text"]
+        }
+        for line in line_timings
+    ]
+
+    json_comment = json.dumps(line_data, ensure_ascii=False)
+    json_comment = escape_ffmetadata_chars(json_comment)
+
+    return json_comment
+
+def merge_chapters_to_standard_audio_file(chapter_files, line_timings=None):
+    """
+    Merge chapter audio files into a standard M4A with jumpable chapters.
+    Converts WAV/PCM/AAC chapters to AAC/M4A before concatenation.
+    Uses original filenames (without extension) as chapter titles.
 
     Args:
-        chapter_files (list): A list of the paths to the individual chapter audio files.
+        chapter_files (list): A list of the names of individual chapter audio files (inside temp_audio).
     """
-    file_list_path = "chapter_list.txt"
-    
-    # Write the list of chapter files to a text file (ffmpeg input)
-    with open(file_list_path, "w", encoding='utf-8') as f:
-        for chapter in chapter_files:
-            chapter_path = os.path.join('temp_audio', chapter)
-            # Validate each chapter file
-            if not validate_file_path(chapter_path):
-                raise ValueError(f"Invalid chapter file: {chapter}")
-            f.write(f"file '{chapter_path}'\n")
+    if not chapter_files:
+        raise ValueError("No chapter files provided.")
 
-    # Construct the output file path
-    output_file = "generated_audiobooks/audiobook.m4a"
+    os.makedirs("generated_audiobooks", exist_ok=True)
 
-    # Validate file list exists
-    if not validate_file_path(file_list_path):
-        raise ValueError("Chapter list file is invalid")
+    # Temporary folder for converted M4A chapters
+    temp_dir = tempfile.mkdtemp(prefix="chapters_")
+    converted_files = []
 
-    # Construct secure ffmpeg command
-    ffmpeg_cmd = [
-        "ffmpeg", "-y", 
-        "-f", "concat", "-safe", "0", 
-        "-i", file_list_path, 
-        "-c", "copy", 
+    # 1. Convert all chapters to M4A (if needed)
+    for chapter in chapter_files:
+        input_path = os.path.join("temp_audio", chapter)
+        if not os.path.exists(input_path):
+            raise ValueError(f"Invalid chapter file: {chapter}")
+
+        output_path = os.path.join(temp_dir, os.path.splitext(chapter)[0] + ".m4a")
+
+        if chapter.lower().endswith(".wav"):
+            create_m4a_file_from_wav_file(input_path, output_path)
+        elif chapter.lower().endswith(".aac"):
+            create_m4a_file_from_raw_aac_file(input_path, output_path)
+        elif chapter.lower().endswith(".m4a"):
+            shutil.copy(input_path, output_path)
+        else:
+            raise ValueError(f"Unsupported chapter format: {chapter}")
+
+        converted_files.append(output_path)
+
+    # 2. Create concat list
+    concat_file = os.path.join(temp_dir, "chapter_list.txt")
+    with open(concat_file, "w", encoding="utf-8") as f:
+        for path in converted_files:
+            f.write(f"file '{path}'\n")
+
+    # 3. Merge into one M4A
+    temp_merged = os.path.join(temp_dir, "temp_merged.m4a")
+    subprocess.run([
+        "ffmpeg", "-y",
+        "-f", "concat", "-safe", "0",
+        "-i", concat_file,
+        "-c", "copy",
+        temp_merged
+    ], check=True)
+
+    # 4. Create chapter metadata
+    metadata_file = os.path.join("generated_audiobooks/", "metadata.txt")
+    with open(metadata_file, "w", encoding="utf-8") as f:
+        f.write(";FFMETADATA1\n")
+
+        json_comment = generate_line_timings_json(line_timings)
+        if json_comment:
+            print("Embedding line timing information as JSON in metadata comment")
+            f.write(f"comment={json_comment}\n\n")
+
+        start_time = 0
+        for chapter, path in zip(chapter_files, converted_files):
+            # Get duration in seconds
+            result = subprocess.run(
+                ["ffprobe", "-v", "error", "-show_entries",
+                 "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", path],
+                capture_output=True, text=True, check=True
+            )
+            duration = float(result.stdout.strip())
+            end_time = start_time + int(duration * 1000)
+
+            # Use original filename (without extension) as chapter title
+            chapter_title = os.path.splitext(os.path.basename(chapter))[0]
+            f.write(f"[CHAPTER]\nTIMEBASE=1/1000\nSTART={start_time}\nEND={end_time}\ntitle={chapter_title}\n")
+
+            start_time = end_time
+
+    # 5. Attach metadata
+    output_file = "generated_audiobooks/audiobook.m4a"
+    subprocess.run([
+        "ffmpeg", "-y",
+        "-i", temp_merged,
+        "-i", metadata_file,
+        "-map_metadata", "1",
+        "-c", "copy",
         output_file
-    ]
+    ], check=True)
+
+    # 6. Clean up
+    shutil.rmtree(temp_dir)
+
+    print(f"Audiobook created with chapters: {output_file}")
 
-    # Use centralized secure command execution
-    allowed_ffmpeg_commands = ['ffmpeg']
-    result = run_shell_command_secure(ffmpeg_cmd, allowed_ffmpeg_commands)
-    
-    if not result or result.returncode != 0:
-        error_msg = result.stderr if result else "Unknown error"
-        raise RuntimeError(f"FFmpeg failed: {error_msg}")
-        
-    print(f"Audiobook created: {output_file}")
 
-def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir, temp_audio_dir):
+def get_duration(path):
+    result = subprocess.run(
+            [
+                "ffprobe", "-v", "error",
+                "-show_entries", "format=duration",
+                "-of", "default=noprint_wrappers=1:nokey=1",
+                path
+            ],
+            capture_output=True,
+            text=True,
+            check=True
+        )
+    return float(result.stdout.strip())
+
+def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir, temp_audio_dir, line_text, timings = [], silence_duration_ms=0):
     """
     Memory-efficient chapter assembly using FFmpeg instead of PyDub.
     
@@ -719,7 +818,10 @@ def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir
     
     # Create a temporary file list for FFmpeg concat
     file_list_path = os.path.join(temp_audio_dir, f"chapter_list_{chapter_file}.txt")
-    
+    if timings:
+        current_time = timings[-1]['end']
+    else:
+        current_time = 0.0
     try:
         with open(file_list_path, "w", encoding='utf-8') as f:
             for line_index in sorted(line_indices):
@@ -727,7 +829,17 @@ def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir
                 # Use absolute path to avoid issues with FFmpeg concat
                 abs_line_path = os.path.abspath(line_path)
                 f.write(f"file '{abs_line_path}'\n")
-        
+                start_time = current_time
+                current_time =  start_time + get_duration(abs_line_path)
+                timings.append({
+                    "text": line_text[line_index],
+                    "start": start_time,
+                    "end": current_time
+                })
+
+            if line_indices:
+                timings[-1]['end'] += (silence_duration_ms / 1000.0)
+
         # Create the full chapter file path
         chapter_path = os.path.join(temp_audio_dir, chapter_file)
         
-- 
2.36.1.windows.1

