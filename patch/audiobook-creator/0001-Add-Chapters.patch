From 88887201a6863c016edcb5d255e49e30e144ac7c Mon Sep 17 00:00:00 2001
From: Raghav <ragha.nv@gmail.com>
Date: Sat, 11 Oct 2025 16:27:17 -0700
Subject: [PATCH] Add chapters

---
 app.py                            | 251 ++++++-----
 generate_audiobook.py             | 717 ++++++++++++++++--------------
 utils/add_emotion_tags_to_text.py | 412 +++++++++++++----
 utils/audiobook_utils.py          | 507 +++++++++++++--------
 4 files changed, 1176 insertions(+), 711 deletions(-)

diff --git a/app.py b/app.py
index 8035285..1fb17ef 100644
--- a/app.py
+++ b/app.py
@@ -16,9 +16,11 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import shutil
 import gradio as gr
 import os
 import traceback
+import json
 from fastapi import FastAPI
 from book_to_txt import process_book_and_extract_text, save_book
 from identify_characters_and_output_book_to_jsonl import process_book_and_identify_characters
@@ -38,25 +40,44 @@ def validate_book_upload(book_file, book_title):
     """Validate book upload and return a notification"""
     if book_file is None:
         return gr.Warning("Please upload a book file first.")
-    
+
     if not book_title:
         return gr.Warning("Please enter a book title.")
-    
+
     return gr.Info(f"Book '{book_title}' ready for processing.", duration=5)
 
+def validate_knowledge_base(kb_file):
+    """Validate knowledge base JSON file and return a notification"""
+    if kb_file is None:
+        return gr.Info("No knowledge base provided. Character identification will use default method.", duration=5)
+
+    try:
+        with open(kb_file, 'r', encoding='utf-8') as f:
+            kb_data = json.load(f)
+
+        # Basic validation - check if it's a valid JSON structure
+        if not isinstance(kb_data, (dict, list)):
+            return gr.Warning("Knowledge base must be a JSON object or array.")
+
+        return gr.Info(f"Knowledge base loaded successfully! ({len(kb_data) if isinstance(kb_data, list) else len(kb_data.keys())} entries)", duration=5)
+    except json.JSONDecodeError as e:
+        return gr.Warning(f"Invalid JSON format in knowledge base: {str(e)}")
+    except Exception as e:
+        return gr.Warning(f"Error reading knowledge base: {str(e)}")
+
 def text_extraction_wrapper(book_file, text_decoding_option, book_title):
     """Wrapper for text extraction with validation and progress updates"""
     if book_file is None or not book_title:
         yield None
         return gr.Warning("Please upload a book file and enter a title first.")
-    
+
     try:
         last_output = None
         # Pass through all yield values from the original function
         for output in process_book_and_extract_text(book_file, text_decoding_option):
             last_output = output
             yield output  # Yield each progress update
-        
+
         # Final yield with success notification
         yield last_output
         return gr.Info("Text extracted successfully! You can now edit the content.", duration=5)
@@ -76,10 +97,10 @@ def save_book_wrapper(text_content, book_title):
     """Wrapper for saving book with validation"""
     if not text_content:
         return gr.Warning("No text content to save.")
-    
+
     if not book_title:
         return gr.Warning("Please enter a book title before saving.")
-    
+
     try:
         save_book(text_content)
         return gr.Info("üìñ Book saved successfully as 'converted_book.txt'!", duration=10)
@@ -101,7 +122,7 @@ async def identify_characters_wrapper(book_title):
         async for output in process_book_and_identify_characters(book_title):
             last_output = output
             yield output  # Yield each progress update
-        
+
         # Final yield with success notification
         yield gr.Info("Character identification complete! You can now add emotion tags or proceed to audiobook generation.", duration=5)
         yield last_output
@@ -129,7 +150,7 @@ async def add_emotion_tags_wrapper():
         async for output in process_emotion_tags():
             last_output = output
             yield output
-        
+
         # Final yield with success notification
         yield gr.Info("Emotion tags added successfully! You can now generate the audiobook.", duration=5)
         yield last_output
@@ -141,61 +162,68 @@ async def add_emotion_tags_wrapper():
         yield None
         return
 
-async def generate_audiobook_wrapper(voice_type, narrator_gender, output_format, book_file, emotion_tags_processed_state):
+async def generate_audiobook_wrapper(voice_type, narrator_gender, output_format, book_file, emotion_tags_processed_state, kb_file):
     """Wrapper for audiobook generation with validation and progress updates"""
     if book_file is None:
-        yield gr.Warning("Please upload a book file first."), None
-        yield None, None
+        yield gr.Warning("Please upload a book file first."), None, None
+        yield None, None, None
         return
     if not voice_type or not output_format:
-        yield gr.Warning("Please select voice type and output format."), None
-        yield None, None
+        yield gr.Warning("Please select voice type and output format."), None, None
+        yield None, None, None
         return
-    
+
     # Early validation for M4B format
     if output_format == "M4B (Chapters & Cover)":
-        yield gr.Info("Validating book file for M4B audiobook generation..."), None
+        yield gr.Info("Validating book file for M4B audiobook generation..."), None, None
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_file)
-        
+
         if not is_valid:
-            yield gr.Warning(f"‚ùå Book validation failed: {error_message}"), None
-            yield None, None
+            yield gr.Warning(f"‚ùå Book validation failed: {error_message}"), None, None
+            yield None, None, None
             return
-            
-        yield gr.Info(f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"), None
-    
+
+        yield gr.Info(f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"), None, None
+
     # Use session state to determine if emotion tags should be used
     add_emotion_tags = emotion_tags_processed_state
-    
+
     if add_emotion_tags:
-        yield gr.Info("üé≠ Using emotion tags (processed in current session)"), None
+        yield gr.Info("üé≠ Using emotion tags (processed in current session)"), None, None
     else:
-        yield gr.Info("üìñ Using standard narration"), None
-    
+        yield gr.Info("üìñ Using standard narration"), None, None
+
     try:
         last_output = None
         audiobook_path = None
         # Pass through all yield values from the original function
-        async for output in process_audiobook_generation(voice_type, narrator_gender, output_format, book_file, add_emotion_tags):
+        async for output in process_audiobook_generation(voice_type, narrator_gender, output_format, book_file, add_emotion_tags, kb_file):
             last_output = output
-            yield output, None  # Yield each progress update without file path
-        
+            yield output, None, None  # Yield each progress update without file path
+
         # Get the correct file extension based on the output format
         generate_m4b_audiobook_file = True if output_format == "M4B (Chapters & Cover)" else False
         file_extension = "m4b" if generate_m4b_audiobook_file else output_format.lower()
-        
+
         # Set the audiobook file path according to the provided information
         audiobook_path = os.path.join("generated_audiobooks", f"audiobook.{file_extension}")
-        
+        kb_output_path = os.path.join("generated_audiobooks", "knowledge_base.json")
+
+        # Check if knowledge_base.json was created
+        if not os.path.exists(kb_output_path):
+            kb_output_path_return = None
+        else:
+            kb_output_path_return = kb_output_path
+
         # Final yield with success notification and file path
-        yield gr.Info(f"Audiobook generated successfully in {output_format} format! You can now download it in the Download section. Click on the blue download link next to the file name.", duration=10), None
-        yield last_output, audiobook_path
+        yield gr.Info(f"Audiobook generated successfully in {output_format} format! You can now download it in the Download section. Click on the blue download link next to the file name.", duration=10), None, None
+        yield last_output, audiobook_path, kb_output_path_return
         return
     except Exception as e:
         print(e)
         traceback.print_exc()
-        yield gr.Warning(f"Error generating audiobook: {str(e)}"), None
-        yield None, None
+        yield gr.Warning(f"Error generating audiobook: {str(e)}"), None, None
+        yield None, None, None
         return
 
 def update_emotion_tags_status_and_state():
@@ -206,80 +234,80 @@ def update_emotion_tags_status_and_state():
 with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     gr.Markdown("# üìñ Audiobook Creator")
     gr.Markdown("Create professional audiobooks from your ebooks in just a few steps.")
-    
+
     # Session state to track if emotion tags were processed
     emotion_tags_processed = gr.State(False)
-    
+
     # Get TTS configuration from environment variables
     current_tts_engine = os.environ.get("TTS_MODEL", "kokoro").lower()
     tts_base_url = os.environ.get("TTS_BASE_URL", "Not configured")
-    
+
     with gr.Row():
         with gr.Column(scale=1):
             gr.Markdown('<div class="step-heading">üìö Step 1: Book Details</div>')
-            
+
             book_title = gr.Textbox(
-                label="Book Title", 
+                label="Book Title",
                 placeholder="Enter the title of your book",
                 info="This will be used for finding the protagonist of the book in the character identification step"
             )
-            
+
             book_input = gr.File(
                 label="Upload Book"
             )
-            
+
             text_decoding_option = gr.Radio(
-                ["textract", "calibre"], 
-                label="Text Extraction Method", 
+                ["textract", "calibre"],
+                label="Text Extraction Method",
                 value="textract",
                 info="Use calibre for better formatted results, wider compatibility for ebook formats. You can try both methods and choose based on the output result."
             )
-            
+
             validate_btn = gr.Button("Validate Book", variant="primary")
 
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">‚úÇÔ∏è Step 2: Extract & Edit Content</div>')
-            
+
             convert_btn = gr.Button("Extract Text", variant="primary")
-            
+
             with gr.Accordion("Editing Tips", open=True):
                 gr.Markdown("""
                 * Remove unwanted sections: Table of Contents, About the Author, Acknowledgements
                 * Fix formatting issues or OCR errors
                 * Check for chapter breaks and paragraph formatting
                 """)
-            
+
             # Navigation buttons for the textbox
             with gr.Row():
                 top_btn = gr.Button("‚Üë Go to Top", size="sm", variant="secondary")
                 bottom_btn = gr.Button("‚Üì Go to Bottom", size="sm", variant="secondary")
-            
+
             text_output = gr.Textbox(
-                label="Edit Book Content", 
+                label="Edit Book Content",
                 placeholder="Extracted text will appear here for editing",
-                interactive=True, 
+                interactive=True,
                 lines=15,
                 elem_id="text_editor"
             )
-            
+
             save_btn = gr.Button("Save Edited Text", variant="primary")
 
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">üß© Step 3: Character Identification (Optional - Requires LLM)</div>')
-            
+
             identify_btn = gr.Button("Identify Characters", variant="primary")
-            
+
             with gr.Accordion("Why Identify Characters?", open=True):
                 gr.Markdown("""
                 * Improves multi-voice narration by assigning different voices to characters
                 * Creates more engaging audiobooks with distinct character voices
                 * Skip this step if you prefer single-voice narration
                 """)
-                
+
             character_output = gr.Textbox(
-                label="Character Identification Progress", 
+                label="Character Identification Progress",
                 placeholder="Character identification progress will be shown here",
                 interactive=False,
                 lines=3
@@ -290,9 +318,9 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     with gr.Row(visible=emotion_tags_visible):
         with gr.Column():
             gr.Markdown('<div class="step-heading">üé≠ Step 3.5: Add Emotion Tags (Optional - Requires LLM)</div>')
-            
+
             emotion_tags_btn = gr.Button("Add Emotion Tags", variant="primary")
-            
+
             with gr.Accordion("What are Emotion Tags?", open=True):
                 gr.Markdown("""
                 **Emotion Tags enhance your audiobook by adding natural expressions:**
@@ -305,12 +333,12 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
                 * **`<groan>`** - For groaning sounds expressing discomfort/frustration
                 * **`<yawn>`** - For yawning or expressions of tiredness
                 * **`<gasp>`** - For gasping sounds of surprise/shock
-                
+
                 These tags are automatically placed based on the text context and work only with **Orpheus TTS**.
                 """)
-                
+
             emotion_tags_output = gr.Textbox(
-                label="Emotion Tags Processing Progress", 
+                label="Emotion Tags Processing Progress",
                 placeholder="Emotion tags processing progress will be shown here",
                 interactive=False,
                 lines=3
@@ -319,39 +347,47 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
     with gr.Row():
         with gr.Column():
             gr.Markdown('<div class="step-heading">üéß Step 4: Generate Audiobook</div>')
-            
+
+            # Knowledge base file input
+            kb_input = gr.File(
+                label="Knowledge Base (Optional)",
+                file_types=[".json", ".txt"]
+            )
+
+            validate_kb_btn = gr.Button("Validate Knowledge Base", variant="secondary", size="sm")
+
             with gr.Row():
                 voice_type = gr.Radio(
-                    ["Single Voice", "Multi-Voice"], 
+                    ["Single Voice", "Multi-Voice"],
                     label="Narration Type",
                     value="Single Voice",
                     info="Multi-Voice requires character identification"
                 )
 
                 narrator_gender = gr.Radio(
-                    ["male", "female"], 
+                    ["male", "female"],
                     label="Choose whether you want the book to be read in a male or female voice",
                     value="female"
                 )
-                
+
                 tts_engine_display = gr.Radio(
-                    ["kokoro", "orpheus"], 
+                    ["kokoro", "orpheus"],
                     label="TTS Engine",
                     value=current_tts_engine,
                     interactive=False,
                     info="Configure TTS engine in .env file. Orpheus supports emotion tags."
                 )
-                
+
                 output_format = gr.Dropdown(
-                    ["M4B (Chapters & Cover)", "AAC", "M4A", "MP3", "WAV", "OPUS", "FLAC", "PCM"], 
+                    ["M4B (Chapters & Cover)", "AAC", "M4A", "MP3", "WAV", "OPUS", "FLAC", "PCM"],
                     label="Output Format",
-                    value="M4B (Chapters & Cover)",
+                    value="M4A",
                     info="M4B supports chapters and cover art"
                 )
-            
+
             # Emotion tags status display (conditional visibility based on TTS engine in .env)
             emotion_tags_visible = current_tts_engine == "orpheus"
-            
+
             with gr.Group(visible=emotion_tags_visible) as emotion_tags_group:
                 emotion_tags_status_display = gr.Radio(
                     choices=["‚úÖ Emotion tags processed - will be used in audiobook", "‚ùå No emotion tags - standard narration will be used"],
@@ -360,56 +396,65 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
                     interactive=False,
                     info="This will update automatically when you process emotion tags in Step 3.5"
                 )
-            
+
             generate_btn = gr.Button("Generate Audiobook", variant="primary")
-            
+
             audio_output = gr.Textbox(
-                label="Generation Progress", 
+                label="Generation Progress",
                 placeholder="Generation progress will be shown here",
                 interactive=False,
                 lines=3
             )
-            
-            # Add a new File component for downloading the audiobook
+
+            # Add a new Group for downloading files
             with gr.Group(visible=False) as download_box:
-                gr.Markdown("### üì• Download Your Audiobook")
+                gr.Markdown("### üì• Download Your Files")
+
+                # File component for audiobook
                 audiobook_file = gr.File(
                     label="Download Generated Audiobook",
                     interactive=False,
                     type="filepath"
                 )
-    
+
+                # File component for updated knowledge base
+                knowledge_base_file = gr.File(
+                    label="Download Updated Knowledge Base",
+                    interactive=False,
+                    type="filepath"
+                )
+
     # Connections with proper handling of Gradio notifications
     validate_btn.click(
-        validate_book_upload, 
-        inputs=[book_input, book_title], 
+        validate_book_upload,
+        inputs=[book_input, book_title],
         outputs=[]
     )
-    
+
     convert_btn.click(
-        text_extraction_wrapper, 
-        inputs=[book_input, text_decoding_option, book_title], 
+        text_extraction_wrapper,
+        inputs=[book_input, text_decoding_option, book_title],
         outputs=[text_output],
         queue=True
     )
-    
+
     save_btn.click(
-        save_book_wrapper, 
-        inputs=[text_output, book_title], 
+        save_book_wrapper,
+        inputs=[text_output, book_title],
         outputs=[],
         queue=True
     )
-    
+
     identify_btn.click(
-        identify_characters_wrapper, 
-        inputs=[book_title], 
+        identify_characters_wrapper,
+        inputs=[book_title],
         outputs=[character_output],
         queue=True
     )
-    
+
     emotion_tags_btn.click(
-        add_emotion_tags_wrapper, 
-        inputs=[], 
+        add_emotion_tags_wrapper,
+        inputs=[],
         outputs=[emotion_tags_output],
         queue=True
     ).then(
@@ -418,20 +463,26 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
         inputs=[],
         outputs=[emotion_tags_status_display, emotion_tags_processed]
     )
-    
+
+    validate_kb_btn.click(
+        validate_knowledge_base,
+        inputs=[kb_input],
+        outputs=[]
+    )
+
     # Update the generate_audiobook_wrapper to output both progress text and file path
     generate_btn.click(
-        generate_audiobook_wrapper, 
-        inputs=[voice_type, narrator_gender, output_format, book_input, emotion_tags_processed], 
-        outputs=[audio_output, audiobook_file],
+        generate_audiobook_wrapper,
+        inputs=[voice_type, narrator_gender, output_format, book_input, emotion_tags_processed, kb_input],
+        outputs=[audio_output, audiobook_file, knowledge_base_file],
         queue=True
     ).then(
         # Make the download box visible after generation completes successfully
-        lambda x: gr.update(visible=True) if x is not None else gr.update(visible=False),
-        inputs=[audiobook_file],
+        lambda audio_path, kb_path: gr.update(visible=True) if audio_path is not None and kb_path is not None else gr.update(visible=False),
+        inputs=[audiobook_file, knowledge_base_file],
         outputs=[download_box]
     )
-    
+
     # Navigation button functionality for textbox scrolling
     top_btn.click(
         None,
@@ -446,7 +497,7 @@ with gr.Blocks(css=css, theme=gr.themes.Default()) as gradio_app:
         }
         """
     )
-    
+
     bottom_btn.click(
         None,
         inputs=[],
@@ -465,4 +516,4 @@ app = gr.mount_gradio_app(app, gradio_app, path="/")  # Mount Gradio at root
 
 if __name__ == "__main__":
     import uvicorn
-    uvicorn.run(app, host="0.0.0.0", port=7860)
\ No newline at end of file
+    uvicorn.run(app, host="0.0.0.0", port=7860)
diff --git a/generate_audiobook.py b/generate_audiobook.py
index 1711db2..3b67218 100755
--- a/generate_audiobook.py
+++ b/generate_audiobook.py
@@ -16,17 +16,21 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import unicodedata
 import shutil
 from openai import OpenAI, AsyncOpenAI
 from tqdm import tqdm
 import json
+import copy
 import os
 import asyncio
 import re
 import tempfile
+from utils.add_emotion_tags_to_text import identify_entities
 from word2number import w2n
 import time
 import sys
+import traceback
 from pydub import AudioSegment
 from utils.run_shell_commands import check_if_ffmpeg_is_installed, check_if_calibre_is_installed
 from utils.file_utils import read_json, empty_directory
@@ -56,39 +60,45 @@ def sanitize_filename(text):
     text = text.replace("'", '').replace('"', '').replace('/', ' ').replace('.', ' ')
     text = text.replace(':', '').replace('?', '').replace('\\', '').replace('|', '')
     text = text.replace('*', '').replace('<', '').replace('>', '').replace('&', 'and')
-    
-    # Normalize whitespace and trim
-    text = ' '.join(text.split())
-    
+    text = text.replace('!', '').replace('@', '').replace('#', '').replace('$', '').replace('%', '')
+    text = text.replace('^', '').replace('(', '').replace(')', '').replace('[', '').replace(']', '')
+    text = text.replace('{', '').replace('}', '').replace('+', '').replace('=', '')
+    text = text.replace(',', '_').replace(' ', '_')
+
+    # Normalize non-ASCII to closest ASCII equivalent
+    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')
+
+    # Collapse multiple underscores
+    text = re.sub(r'_+', '_', text).strip('_')
     return text
 
 def is_only_punctuation(text):
     """
     Check if a line contains only punctuation marks without any actual words.
     This helps avoid TTS errors when encountering lines with just punctuation.
-    
+
     Args:
         text (str): The text line to check
-        
+
     Returns:
         bool: True if the line contains only punctuation, False otherwise
     """
     # Remove all whitespace
     cleaned_text = text.strip()
-    
+
     # If empty after stripping, it's not useful for TTS
     if not cleaned_text:
         return True
-    
+
     # Import string for standard punctuation
     import string
-    
+
     # Extended punctuation set including common Unicode punctuation in books
     extended_punctuation = string.punctuation + '‚Äî‚Äì""''‚Ä¶‚Äö‚Äû‚Äπ‚Ä∫¬´¬ª‚Ä∞‚Ä±'
-    
+
     # Remove all punctuation marks (both ASCII and extended Unicode)
     text_without_punct = ''.join(char for char in cleaned_text if char not in extended_punctuation)
-    
+
     # If nothing remains after removing punctuation, it's only punctuation
     return len(text_without_punct.strip()) == 0
 
@@ -130,13 +140,13 @@ def check_if_chapter_heading(text):
         except ValueError:
             return False  # Invalid number format
     return False  # No match
-    
+
 def find_voice_for_gender_score(character: str, character_gender_map, engine_name: str, narrator_gender: str):
     """
     Finds the appropriate voice for a character based on their gender score using the new voice mapping system.
 
     This function takes in the name of a character, a dictionary mapping character names to their gender scores,
-    the TTS engine name, and the narrator gender preference. It returns the voice identifier that matches 
+    the TTS engine name, and the narrator gender preference. It returns the voice identifier that matches
     the character's gender score within the appropriate score map (male_score_map or female_score_map).
 
     Args:
@@ -157,7 +167,7 @@ def find_voice_for_gender_score(character: str, character_gender_map, engine_nam
     if "scores" in character_gender_map and character.lower() in character_gender_map["scores"]:
         character_info = character_gender_map["scores"][character.lower()]
         character_gender_score = character_info["gender_score"]
-        
+
         return get_voice_for_character_score(engine_name, narrator_gender, character_gender_score)
     else:
         # Fallback for unknown characters - use score 5 (neutral)
@@ -166,16 +176,16 @@ def find_voice_for_gender_score(character: str, character_gender_map, engine_nam
 def validate_book_for_m4b_generation(book_path):
     """
     Validates that the book file is suitable for M4B audiobook generation.
-    
+
     This function performs early validation to catch issues before audio generation:
     - Checks if the book file path is safe and accessible
     - Verifies that ebook-meta command is available
     - Tests metadata extraction from the book
     - Ensures cover image can be extracted
-    
+
     Args:
         book_path (str): Path to the book file
-        
+
     Returns:
         tuple: (is_valid, error_message, metadata)
             - is_valid (bool): True if validation passed
@@ -186,20 +196,20 @@ def validate_book_for_m4b_generation(book_path):
         # Validate file path safety and existence
         if not validate_file_path(book_path):
             return False, f"Invalid or inaccessible book file: {book_path}. Please check the file path and permissions.", None
-        
+
         # Test metadata extraction (this also validates ebook-meta availability)
         metadata = get_ebook_metadata_with_cover(book_path)
-        
+
         # Check if we got meaningful metadata
         if not metadata or len(metadata) == 0:
             return False, f"No metadata could be extracted from the book file: {book_path}. Please ensure it's a valid ebook format.", None
-            
+
         # Check if cover extraction worked (cover.jpg should exist after get_ebook_metadata_with_cover)
         if not validate_file_path("cover.jpg"):
             return False, f"Could not extract cover image from the book file: {book_path}. The book may not contain a cover image.", None
-            
+
         return True, None, metadata
-        
+
     except ValueError as e:
         return False, f"Book file validation error: {str(e)}", None
     except RuntimeError as e:
@@ -207,7 +217,245 @@ def validate_book_for_m4b_generation(book_path):
     except Exception as e:
         return False, f"Unexpected error during book validation: {str(e)}", None
 
-async def generate_audio_with_single_voice(output_format, narrator_gender, generate_m4b_audiobook_file=False, book_path="", add_emotion_tags=False):
+
+async def identify_chapter_entities(chapter_text, kb):
+    def extract_entities(chapter_text, kb_entities):
+        entities_in_chapter = []
+        for entity in kb_entities:
+            # Simple word boundary check, case-sensitive
+            if re.search(rf"\b{re.escape(entity)}\b", chapter_text):
+                entities_in_chapter.append(entity)
+        return entities_in_chapter
+
+    entities_in_chapter = extract_entities(chapter_text, kb.keys())
+    filtered_kb = {name: kb[name] for name in entities_in_chapter}
+    chapter_number, entities_data, retries = await identify_entities(chapter_text, filtered_kb)
+
+    # Build new partial knowledge base
+    new_kb = {}
+    for entity, info in entities_data.items():
+        if entity in kb:
+            first_seen = kb[entity].get("firstSeenChapter", chapter_number)
+        else:
+            first_seen = chapter_number
+        new_kb[entity] = {
+            "description": info["description"],
+            "firstSeenChapter": first_seen,
+            "lastSeenChapter": chapter_number
+        }
+
+    return new_kb, retries
+
+async def process_chapters(lines, knowledge_base_file):
+    chapters = split_lines_into_chapters(lines)
+    chapter_to_output = {}
+    with open(knowledge_base_file, 'r') as f:
+        knowledge_base = json.load(f)
+
+    chapter_to_output["__original_kb__"] = copy.deepcopy(knowledge_base)
+    max_retries_allowed = len(chapters) * 2
+    for chapter in chapters:
+
+        try:
+            entities, retries = await identify_chapter_entities(chapter["chapter_text"], knowledge_base) #{"Han Jue": {"description": chapter["chapter_text"]}}
+            chapter_title = os.path.splitext(os.path.basename(chapter["chapter_filename"]))[0]
+            chapter_to_output[chapter_title] = entities
+            knowledge_base.update(entities)
+            max_retries_allowed -= retries
+
+            if max_retries_allowed <= 0:
+                raise Exception("Max retries reached.")
+        except Exception as ve:
+            traceback.print_exc()
+            break
+
+    return chapter_to_output, knowledge_base
+
+def split_lines_into_chapters(lines):
+    chapters = []
+    current_chapter_lines = []
+    current_chapter_filename = "Introduction.wav"
+
+    for line in lines:
+        if check_if_chapter_heading(line):
+            if current_chapter_lines:
+                chapters.append({
+                    "chapter_filename": current_chapter_filename,
+                    "chapter_text": "\n".join(current_chapter_lines)
+                })
+            current_chapter_filename = f"{sanitize_filename(line)}.wav"
+            current_chapter_lines = []
+
+        current_chapter_lines.append(line)
+
+    # Add the last chapter
+    if current_chapter_lines:
+        chapters.append({
+            "chapter_filename": current_chapter_filename,
+            "chapter_text": "\n".join(current_chapter_lines)
+        })
+
+    return chapters
+
+async def process_audio_results_and_generate_audiobook(
+    lines,
+    task_to_index,
+    tasks,
+    temp_line_audio_dir,
+    temp_audio_dir,
+    chapter_files,
+    output_format,
+    generate_m4b_audiobook_file,
+    book_path,
+    progress_bar,
+    kb_file
+):
+    """
+    Common function to process audio results and generate the final audiobook.
+    Extracted from both generate_audio_with_single_voice and generate_audio_with_multiple_voices.
+    """
+    # Initialize chapter tracking variables
+    chapter_index = 1
+    current_chapter_audio = "Introduction.wav"
+    chapter_line_map = {}
+    total_lines = len(lines)
+    results_all = [None] * total_lines
+
+    if kb_file:
+        chapter_entities, final_knowledge_base = await process_chapters(lines, kb_file)
+
+        kb_output_path = os.path.join("generated_audiobooks", "knowledge_base.json")
+        with open(kb_output_path, "w", encoding="utf-8") as f:
+            json.dump(final_knowledge_base, f, ensure_ascii=False, indent=4)
+    else:
+        chapter_entities = None
+
+    # Process tasks with progress updates
+    last_reported = -1
+    latest_chapter_name = None
+    progress_counter = 0
+    while tasks:
+        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
+
+        # Store results as tasks complete
+        for completed_task in done:
+            idx = task_to_index[completed_task]
+            result = completed_task.result()
+            results_all[idx] = result
+
+            progress_counter += 1
+
+            if result and result["is_chapter_heading"]:
+                latest_chapter_name = result['line']
+
+        tasks = list(pending)
+
+        # Only yield if the counter has changed
+        if progress_counter > last_reported:
+            last_reported = progress_counter
+            percent = (progress_counter / total_lines) * 100
+            yield f"Generating audiobook. Chapter: {latest_chapter_name or 'N/A'}. Progress: {percent:.1f}%"
+
+    # All tasks have completed at this point and results_all is populated
+    results = [r for r in results_all if r is not None]  # Filter out empty lines
+
+    progress_bar.close()
+
+    # Filter out empty lines (same as in your original code)
+    results = [r for r in results_all if r is not None]
+
+    yield "Completed generating audio for all lines"
+
+    # Second pass: Organize by chapters
+    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
+    yield "Organizing lines into chapters"
+
+    chapter_line_text_map = {}
+    for result in sorted(results, key=lambda x: x["index"]):
+        # Check if this is a chapter heading
+        if result["is_chapter_heading"]:
+            chapter_index += 1
+            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
+
+        if current_chapter_audio not in chapter_files:
+            chapter_files.append(current_chapter_audio)
+            chapter_line_map[current_chapter_audio] = []
+
+        # Add this line index to the chapter
+        chapter_line_map[current_chapter_audio].append(result["index"])
+        chapter_line_text_map[result["index"]] = result["line"]
+        chapter_organization_bar.update(1)
+
+    chapter_organization_bar.close()
+    yield f"Organized {len(results)} lines into {len(chapter_files)} chapters"
+
+    # Third pass: Concatenate audio files for each chapter in order
+    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
+
+    SILENCE_DURATION_MS = 1000
+    line_timings = []
+    for chapter_file in chapter_files:
+        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
+        assemble_chapter_with_ffmpeg(
+            chapter_file,
+            chapter_line_map[chapter_file],
+            temp_line_audio_dir,
+            temp_audio_dir,
+            chapter_line_text_map,
+            line_timings,
+            SILENCE_DURATION_MS
+        )
+
+        chapter_assembly_bar.update(1)
+        yield f"Assembled chapter: {chapter_file}"
+
+    chapter_assembly_bar.close()
+    yield "Completed assembling all chapters"
+
+    # Post-processing steps
+    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
+
+    # Add silence to each chapter file using FFmpeg
+    for chapter_file in chapter_files:
+        chapter_path = os.path.join(temp_audio_dir, chapter_file)
+
+        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
+        add_silence_to_chapter_with_ffmpeg(chapter_path, SILENCE_DURATION_MS)  # 1 second silence
+
+        post_processing_bar.update(1)
+        yield f"Added silence to chapter: {chapter_file}"
+
+    m4a_chapter_files = []
+
+    # Convert all chapter files to M4A format
+    for chapter_file in chapter_files:
+        chapter_name = chapter_file.split('.')[0]
+        m4a_chapter_files.append(f"{chapter_name}.m4a")
+        # Convert WAV to M4A for better compatibility with timestamps and metadata
+        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
+        post_processing_bar.update(1)
+        yield f"Converted chapter to M4A: {chapter_name}"
+
+    post_processing_bar.close()
+
+    # Clean up temp line audio files
+    yield "Cleaning up temporary files"
+    shutil.rmtree(temp_line_audio_dir)
+    yield "Temporary files cleanup complete"
+
+    if generate_m4b_audiobook_file:
+        # Merge all chapter files into a final m4b audiobook
+        yield "Creating M4B audiobook file..."
+        merge_chapters_to_m4b(book_path, m4a_chapter_files)
+        yield "M4B audiobook created successfully"
+    else:
+        # Merge all chapter files into a standard M4A audiobook
+        yield "Creating final audiobook..."
+        merge_chapters_to_standard_audio_file(m4a_chapter_files, line_timings, chapter_entities)
+        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
+        yield f"Audiobook in {output_format} format created successfully"
+
+async def generate_audio_with_single_voice(output_format, narrator_gender, generate_m4b_audiobook_file=False, book_path="", add_emotion_tags=False, kb_file=None):
     # Read the text from the file
     """
     Generate an audiobook using a single voice for narration and dialogues.
@@ -228,15 +476,15 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
         str: Progress updates as the audiobook generation progresses through loading text, generating audio,
              organizing by chapters, assembling chapters, and post-processing steps.
     """
-    
+
     # Early validation for M4B generation
     if generate_m4b_audiobook_file:
         yield "Validating book file for M4B audiobook generation..."
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             raise ValueError(f"‚ùå Book validation failed: {error_message}")
-            
+
         yield f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"
 
     # Check if emotion tags should be used and if they have been pre-applied
@@ -247,20 +495,20 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
     else:
         with open("converted_book.txt", "r", encoding='utf-8') as f:
             text = f.read()
-        
+
         # Apply text preprocessing for Orpheus TTS to prevent repetition issues
         if TTS_MODEL.lower() == "orpheus":
             text = preprocess_text_for_tts(text)
             yield "Applied text preprocessing for Orpheus TTS"
-    
+
     lines = text.split("\n")
-    
+
     # Filter out empty lines
     lines = [line.strip() for line in lines if line.strip()]
-    
+
     # Set the voices to be used - now using the new voice mapping system
     narrator_voice, dialogue_voice = get_narrator_and_dialogue_voices(
-        engine_name=TTS_MODEL, 
+        engine_name=TTS_MODEL,
         narrator_gender=narrator_gender
     )
 
@@ -272,41 +520,32 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
 
     os.makedirs(temp_audio_dir, exist_ok=True)
     os.makedirs(temp_line_audio_dir, exist_ok=True)
-    
+
     # Batch processing parameters
     semaphore = asyncio.Semaphore(TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE)
-    
+
     # Initial setup for chapters
-    chapter_index = 1
-    current_chapter_audio = f"Introduction.wav"
     chapter_files = []
-    
+
     # First pass: Generate audio for each line independently
     total_size = len(lines)
 
-    progress_counter = 0
-    
     # For tracking progress with tqdm in an async context
     progress_bar = tqdm(total=total_size, unit="line", desc="Audio Generation Progress")
-    
-    # Maps chapters to their line indices
-    chapter_line_map = {}
-    
+
     async def process_single_line(line_index, line):
         async with semaphore:
-            nonlocal progress_counter
 
             if not line or is_only_punctuation(line):
                 progress_bar.update(1)
-                progress_counter += 1
                 return None
-                
+
             # Split the line into annotated parts
             annotated_parts = split_and_annotate_text(line)
-            
+
             # Create combined audio using PyDub for seamless concatenation
             combined_audio = AudioSegment.empty()
-            
+
             for part in annotated_parts:
                 text_to_speak = part["text"].strip()
 
@@ -314,54 +553,52 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
                     continue
 
                 voice_to_speak_in = narrator_voice if part["type"] == "narration" else dialogue_voice
-                
+
                 # Create temporary file for this part
                 temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                 temp_path = temp_file.name
                 temp_file.close()
-                
+
                 try:
                     # Generate audio for the part using retry mechanism
                     audio_buffer = await generate_audio_with_retry(
-                        async_openai_client, 
+                        async_openai_client,
                         TTS_MODEL,
-                        text_to_speak, 
+                        text_to_speak,
                         voice_to_speak_in
                     )
-                    
+
                     # Write part audio to temp file
                     with open(temp_path, "wb") as temp_wav:
                         temp_wav.write(audio_buffer)
-                    
+
                     # Load as AudioSegment and add to combined audio
                     part_segment = AudioSegment.from_wav(temp_path)
                     combined_audio += part_segment
-                    
+
                 except Exception as e:
                     # Log the error for debugging
                     print(f"Warning: Failed to generate audio for text: '{text_to_speak[:50]}...' - Error: {str(e)}")
                     # Skip this part and continue with next part
-                    
+
                 finally:
                     # Always clean up temp file
                     if os.path.exists(temp_path):
                         os.unlink(temp_path)
-            
+
             # Check if we have any audio content before exporting
             if len(combined_audio) == 0:
                 # If no audio was generated for this line, skip it entirely
                 progress_bar.update(1)
-                progress_counter += 1
                 return None
-            
+
             # Write this line's audio to a temporary file
             line_audio_path = os.path.join(temp_line_audio_dir, f"line_{line_index:06d}.wav")
             combined_audio.export(line_audio_path, format="wav")
-            
+
             # Update progress bar
             progress_bar.update(1)
-            progress_counter += 1
-            
+
             return {
                 "index": line_index,
                 "is_chapter_heading": check_if_chapter_heading(line),
@@ -375,129 +612,35 @@ async def generate_audio_with_single_voice(output_format, narrator_gender, gener
         task = asyncio.create_task(process_single_line(i, line))
         tasks.append(task)
         task_to_index[task] = i
-    
-    # Initialize results_all list
-    results_all = [None] * len(lines)
-    
-    # Process tasks with progress updates
-    last_reported = -1
-    while tasks:
-        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
-        
-        # Store results as tasks complete
-        for completed_task in done:
-            idx = task_to_index[completed_task]
-            results_all[idx] = completed_task.result()
-        
-        tasks = list(pending)
-        
-        # Only yield if the counter has changed
-        if progress_counter > last_reported:
-            last_reported = progress_counter
-            percent = (progress_counter / total_size) * 100
-            yield f"Generating audiobook. Progress: {percent:.1f}%"
-    
-    # All tasks have completed at this point and results_all is populated
-    results = [r for r in results_all if r is not None]  # Filter out empty lines
-    
-    progress_bar.close()
-    
-    # Filter out empty lines (same as in your original code)
-    results = [r for r in results_all if r is not None]
-    
-    yield "Completed generating audio for all lines"
 
-    # Second pass: Organize by chapters
-    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
-    
-    for result in sorted(results, key=lambda x: x["index"]):
-        # Check if this is a chapter heading
-        if result["is_chapter_heading"]:
-            chapter_index += 1
-            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
-            
-        if current_chapter_audio not in chapter_files:
-            chapter_files.append(current_chapter_audio)
-            chapter_line_map[current_chapter_audio] = []
-            
-        # Add this line index to the chapter
-        chapter_line_map[current_chapter_audio].append(result["index"])
-        chapter_organization_bar.update(1)
-    
-    chapter_organization_bar.close()
-    yield "Organizing audio by chapters complete"
-    
-    # Third pass: Concatenate audio files for each chapter in order
-    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
-    
-    for chapter_file in chapter_files:
-        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
-        assemble_chapter_with_ffmpeg(
-            chapter_file, 
-            chapter_line_map[chapter_file], 
-            temp_line_audio_dir, 
-            temp_audio_dir
-        )
-        
-        chapter_assembly_bar.update(1)
-        yield f"Assembled chapter: {chapter_file}"
-    
-    chapter_assembly_bar.close()
-    yield "Completed assembling all chapters"
-    
-    # Post-processing steps
-    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
-    
-    # Add silence to each chapter file using FFmpeg
-    for chapter_file in chapter_files:
-        chapter_path = os.path.join(temp_audio_dir, chapter_file)
-        
-        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
-        add_silence_to_chapter_with_ffmpeg(chapter_path, 1000)  # 1 second silence
-        
-        post_processing_bar.update(1)
-        yield f"Added silence to chapter: {chapter_file}"
-
-    m4a_chapter_files = []
-
-    # Convert all chapter files to M4A format
-    for chapter_file in chapter_files:
-        chapter_name = chapter_file.split('.')[0]
-        m4a_chapter_files.append(f"{chapter_name}.m4a")
-        # Convert WAV to M4A for better compatibility with timestamps and metadata
-        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
-        post_processing_bar.update(1)
-        yield f"Converted chapter to M4A: {chapter_name}"
-    
-    post_processing_bar.close()
-    
-    # Clean up temp line audio files
-    shutil.rmtree(temp_line_audio_dir)
-    yield "Cleaned up temporary files"
 
-    if generate_m4b_audiobook_file:
-        # Merge all chapter files into a final m4b audiobook
-        yield "Creating M4B audiobook file..."
-        merge_chapters_to_m4b(book_path, m4a_chapter_files)
-        yield "M4B audiobook created successfully"
-    else:
-        # Merge all chapter files into a standard M4A audiobook
-        yield "Creating final audiobook..."
-        merge_chapters_to_standard_audio_file(m4a_chapter_files)
-        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
-        yield f"Audiobook in {output_format} format created successfully"
+    # Call common processing function
+    async for progress_msg in process_audio_results_and_generate_audiobook(
+        lines,
+        task_to_index,
+        tasks,
+        temp_line_audio_dir,
+        temp_audio_dir,
+        chapter_files,
+        output_format,
+        generate_m4b_audiobook_file,
+        book_path,
+        progress_bar,
+        kb_file
+    ):
+        yield progress_msg
 
 def apply_emotion_tags_to_multi_voice_data(json_data_array):
     """
     Dynamically apply pre-processed emotion tags to multi-voice JSONL data.
-    
+
     This function reads emotion-enhanced text from tag_added_lines_chunks.txt
     and applies it to the speaker-attributed JSONL data in memory, preserving
     speaker attributions while using the enhanced text content.
-    
+
     Args:
         json_data_array (list): Original speaker-attributed JSONL data
-        
+
     Returns:
         tuple: (success, json_data_array, message)
             - success (bool): True if emotion tags were successfully applied
@@ -506,7 +649,7 @@ def apply_emotion_tags_to_multi_voice_data(json_data_array):
     """
     if not os.path.exists("tag_added_lines_chunks.txt"):
         return False, json_data_array, "No pre-processed emotion tags found"
-    
+
     try:
         # Read the enhanced lines from tag_added_lines_chunks.txt
         with open("tag_added_lines_chunks.txt", "r", encoding='utf-8') as f:
@@ -519,11 +662,11 @@ def apply_emotion_tags_to_multi_voice_data(json_data_array):
             return True, json_data_array, "Successfully applied pre-processed emotion tags"
         else:
             return False, json_data_array, f"Line count mismatch: {len(enhanced_lines)} enhanced lines vs {len(json_data_array)} speaker-attributed lines"
-            
+
     except Exception as e:
         return False, json_data_array, f"Error applying emotion tags: {str(e)}"
 
-async def generate_audio_with_multiple_voices(output_format, narrator_gender, generate_m4b_audiobook_file=False, book_path="", add_emotion_tags=False):
+async def generate_audio_with_multiple_voices(output_format, narrator_gender, generate_m4b_audiobook_file=False, book_path="", add_emotion_tags=False, kb_file=None):
     # Path to the JSONL file containing speaker-attributed lines
     """
     Generate an audiobook in the specified format using multiple voices for each line
@@ -546,17 +689,17 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     :param book_path: The path to the book file (required for generating an M4B audiobook file)
     :param add_emotion_tags: Whether to use pre-applied emotion tags in the audiobook. Defaults to False.
     """
-    
+
     # Early validation for M4B generation
     if generate_m4b_audiobook_file:
         yield "Validating book file for M4B audiobook generation..."
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             raise ValueError(f"‚ùå Book validation failed: {error_message}")
-            
+
         yield f"‚úÖ Book validation successful! Title: {metadata.get('Title', 'Unknown')}, Author: {metadata.get('Author(s)', 'Unknown')}"
-    
+
     file_path = 'speaker_attributed_book.jsonl'
     json_data_array = []
 
@@ -587,7 +730,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
             '<yawn>' in item.get('line', '') or '<gasp>' in item.get('line', '')
             for item in json_data_array
         )
-        
+
         if has_emotion_tags:
             yield "Removing existing emotion tags from JSONL data as per user preference"
             import re
@@ -596,7 +739,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
                     # Remove emotion tags from the line
                     line_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*', '', item["line"])
                     item["line"] = line_without_tags
-    
+
     # Apply text preprocessing for Orpheus TTS to prevent repetition issues
     if TTS_MODEL.lower() == "orpheus":
         for item in json_data_array:
@@ -610,7 +753,7 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     # Get narrator voice using the new voice mapping system
     narrator_voice = find_voice_for_gender_score("narrator", character_gender_map, TTS_MODEL, narrator_gender)
     yield "Loaded voice mappings and selected narrator voice"
-    
+
     # Setup directories
     temp_audio_dir = "temp_audio"
     temp_line_audio_dir = os.path.join(temp_audio_dir, "line_segments")
@@ -620,21 +763,13 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
     os.makedirs(temp_audio_dir, exist_ok=True)
     os.makedirs(temp_line_audio_dir, exist_ok=True)
     yield "Set up temporary directories for audio processing"
-    
+
     # Batch processing parameters
     semaphore = asyncio.Semaphore(TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE)
-    
+
     # Initial setup for chapters
-    chapter_index = 1
-    current_chapter_audio = f"Introduction.wav"
     chapter_files = []
-    
-    # First pass: Generate audio for each line independently
-    # and track chapter organization
-    chapter_line_map = {}  # Maps chapters to their line indices
 
-    progress_counter = 0
-    
     # For tracking progress with tqdm in an async context
     total_lines = len(json_data_array)
     progress_bar = tqdm(total=total_lines, unit="line", desc="Audio Generation Progress")
@@ -643,24 +778,22 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
 
     async def process_single_line(line_index, doc):
         async with semaphore:
-            nonlocal progress_counter
 
             line = doc["line"].strip()
 
             if not line or is_only_punctuation(line):
                 progress_bar.update(1)
-                progress_counter += 1
                 return None
 
             speaker = doc["speaker"]
             speaker_voice = find_voice_for_gender_score(speaker, character_gender_map, TTS_MODEL, narrator_gender)
-            
+
             # Split the line into annotated parts
             annotated_parts = split_and_annotate_text(line)
-            
+
             # Create combined audio using PyDub for seamless concatenation
             combined_audio = AudioSegment.empty()
-            
+
             for part in annotated_parts:
                 text_to_speak = part["text"].strip()
 
@@ -668,60 +801,58 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
                     continue
 
                 voice_to_speak_in = narrator_voice if part["type"] == "narration" else speaker_voice
-                
+
                 # Create temporary file for this part
                 temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                 temp_path = temp_file.name
                 temp_file.close()
-                
+
                 try:
                     # Generate audio for the part using retry mechanism
                     audio_buffer = await generate_audio_with_retry(
-                        async_openai_client, 
+                        async_openai_client,
                         TTS_MODEL,
-                        text_to_speak, 
+                        text_to_speak,
                         voice_to_speak_in
                     )
-                    
+
                     # Write part audio to temp file
                     with open(temp_path, "wb") as temp_wav:
                         temp_wav.write(audio_buffer)
-                    
+
                     # Load as AudioSegment and add to combined audio
                     part_segment = AudioSegment.from_wav(temp_path)
                     combined_audio += part_segment
-                    
+
                 except Exception as e:
                     # Log the error for debugging
                     print(f"Warning: Failed to generate audio for text: '{text_to_speak[:50]}...' - Error: {str(e)}")
                     # Skip this part and continue with next part
-                    
+
                 finally:
                     # Always clean up temp file
                     if os.path.exists(temp_path):
                         os.unlink(temp_path)
-            
+
             # Check if we have any audio content before exporting
             if len(combined_audio) == 0:
                 # If no audio was generated for this line, skip it entirely
                 progress_bar.update(1)
-                progress_counter += 1
                 return None
-            
+
             # Write this line's audio to a temporary file
             line_audio_path = os.path.join(temp_line_audio_dir, f"line_{line_index:06d}.wav")
             combined_audio.export(line_audio_path, format="wav")
-            
+
             # Update progress bar
             progress_bar.update(1)
-            progress_counter += 1
-            
+
             return {
                 "index": line_index,
                 "is_chapter_heading": check_if_chapter_heading(line),
                 "line": line
             }
-    
+
     # Create tasks and store them with their index for result collection
     tasks = []
     task_to_index = {}
@@ -729,126 +860,40 @@ async def generate_audio_with_multiple_voices(output_format, narrator_gender, ge
         task = asyncio.create_task(process_single_line(i, doc))
         tasks.append(task)
         task_to_index[task] = i
-    
-    # Initialize results_all list
-    results_all = [None] * len(json_data_array)
-    
-    # Process tasks with progress updates
-    last_reported = -1
-    while tasks:
-        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
-        
-        # Store results as tasks complete
-        for completed_task in done:
-            idx = task_to_index[completed_task]
-            results_all[idx] = completed_task.result()
-        
-        tasks = list(pending)
-        
-        # Only yield if the counter has changed
-        if progress_counter > last_reported:
-            last_reported = progress_counter
-            percent = (progress_counter / total_lines) * 100
-            yield f"Generating audiobook. Progress: {percent:.1f}%"
-    
-    # All tasks have completed at this point and results_all is populated
-    results = [r for r in results_all if r is not None]  # Filter out empty lines
-    
-    progress_bar.close()
-    
-    # Filter out empty lines (same as in your original code)
-    results = [r for r in results_all if r is not None]
-    
-    yield "Completed generating audio for all lines"
-    
-    # Second pass: Organize by chapters
-    chapter_organization_bar = tqdm(total=len(results), unit="result", desc="Organizing Chapters")
-    yield "Organizing lines into chapters"
-    
-    for result in sorted(results, key=lambda x: x["index"]):
-        # Check if this is a chapter heading
-        if result["is_chapter_heading"]:
-            chapter_index += 1
-            current_chapter_audio = f"{sanitize_filename(result['line'])}.wav"
-            
-        if current_chapter_audio not in chapter_files:
-            chapter_files.append(current_chapter_audio)
-            chapter_line_map[current_chapter_audio] = []
-            
-        # Add this line index to the chapter
-        chapter_line_map[current_chapter_audio].append(result["index"])
-        chapter_organization_bar.update(1)
-    
-    chapter_organization_bar.close()
-    yield f"Organized {len(results)} lines into {len(chapter_files)} chapters"
-    
-    # Third pass: Concatenate audio files for each chapter in order
-    chapter_assembly_bar = tqdm(total=len(chapter_files), unit="chapter", desc="Assembling Chapters")
-    
-    for chapter_file in chapter_files:
-        # Use FFmpeg-based assembly instead of PyDub for memory efficiency
-        assemble_chapter_with_ffmpeg(
-            chapter_file, 
-            chapter_line_map[chapter_file], 
-            temp_line_audio_dir, 
-            temp_audio_dir
-        )
-        
-        chapter_assembly_bar.update(1)
-        yield f"Assembled chapter: {chapter_file}"
-    
-    chapter_assembly_bar.close()
-    yield "Completed assembling all chapters"
-    
-    # Post-processing steps
-    post_processing_bar = tqdm(total=len(chapter_files)*2, unit="task", desc="Post Processing")
-    
-    # Add silence to each chapter file using FFmpeg
-    for chapter_file in chapter_files:
-        chapter_path = os.path.join(temp_audio_dir, chapter_file)
-        
-        # Use FFmpeg-based silence addition instead of PyDub for memory efficiency
-        add_silence_to_chapter_with_ffmpeg(chapter_path, 1000)  # 1 second silence
-        
-        post_processing_bar.update(1)
-        yield f"Added silence to chapter: {chapter_file}"
 
-    m4a_chapter_files = []
-
-    # Convert all chapter files to M4A format
-    for chapter_file in chapter_files:
-        chapter_name = chapter_file.split('.')[0]
-        m4a_chapter_files.append(f"{chapter_name}.m4a")
-        # Convert WAV to M4A for better compatibility with timestamps and metadata
-        convert_audio_file_formats("wav", "m4a", temp_audio_dir, chapter_name)
-        post_processing_bar.update(1)
-        yield f"Converted chapter to M4A: {chapter_name}"
-    
-    post_processing_bar.close()
-    
-    # Clean up temp line audio files
-    yield "Cleaning up temporary files"
-    shutil.rmtree(temp_line_audio_dir)
-    yield "Temporary files cleanup complete"
-
-    if generate_m4b_audiobook_file:
-        # Merge all chapter files into a final m4b audiobook
-        yield "Creating M4B audiobook file..."
-        merge_chapters_to_m4b(book_path, m4a_chapter_files)
-        yield "M4B audiobook created successfully"
-    else:
-        # Merge all chapter files into a standard M4A audiobook
-        yield "Creating final audiobook..."
-        merge_chapters_to_standard_audio_file(m4a_chapter_files)
-        convert_audio_file_formats("m4a", output_format, "generated_audiobooks", "audiobook")
-        yield f"Audiobook in {output_format} format created successfully"
-
-async def process_audiobook_generation(voice_option, narrator_gender, output_format, book_path, add_emotion_tags=False):
+    # Call common processing function
+    async for progress_msg in process_audio_results_and_generate_audiobook(
+        json_data_array,
+        task_to_index,
+        tasks,
+        temp_line_audio_dir,
+        temp_audio_dir,
+        chapter_files,
+        output_format,
+        generate_m4b_audiobook_file,
+        book_path,
+        progress_bar,
+        kb_file
+    ):
+        yield progress_msg
+
+async def process_audiobook_generation(voice_option, narrator_gender, output_format, book_path, add_emotion_tags=False, kb_file=None):
     is_audio_generator_api_up, message = await check_if_audio_generator_api_is_up(async_openai_client)
 
     if not is_audio_generator_api_up:
         raise Exception(message)
 
+    # Load knowledge base if provided
+    kb_data = None
+    if kb_file:
+        try:
+            import json
+            with open(kb_file, 'r', encoding='utf-8') as f:
+                kb_data = json.load(f)
+            yield f"üìö Knowledge base loaded with {len(kb_data) if isinstance(kb_data, list) else len(kb_data.keys())} entries"
+        except Exception as e:
+            yield f"‚ö†Ô∏è Warning: Could not load knowledge base: {str(e)}"
+
     generate_m4b_audiobook_file = False
 
     if output_format == "M4B (Chapters & Cover)":
@@ -858,16 +903,16 @@ async def process_audiobook_generation(voice_option, narrator_gender, output_for
         if voice_option == "Single Voice":
             yield "\nüéß Generating audiobook with a **single voice**..."
             await asyncio.sleep(1)
-            async for line in generate_audio_with_single_voice(output_format.lower(), narrator_gender, generate_m4b_audiobook_file, book_path, add_emotion_tags):
+            async for line in generate_audio_with_single_voice(output_format.lower(), narrator_gender, generate_m4b_audiobook_file, book_path, add_emotion_tags, kb_file):
                 yield line
         elif voice_option == "Multi-Voice":
             yield "\nüé≠ Generating audiobook with **multiple voices**..."
             await asyncio.sleep(1)
-            async for line in generate_audio_with_multiple_voices(output_format.lower(), narrator_gender, generate_m4b_audiobook_file, book_path, add_emotion_tags):
+            async for line in generate_audio_with_multiple_voices(output_format.lower(), narrator_gender, generate_m4b_audiobook_file, book_path, add_emotion_tags, kb_file):
                 yield line
 
         yield f"\nüéß Audiobook is generated ! You can now download it in the Download section below. Click on the blue download link next to the file name."
-        
+
     except ValueError as e:
         # Handle validation errors specifically
         error_msg = str(e)
@@ -910,7 +955,7 @@ async def main():
         if not is_calibre_installed:
             print("‚ö†Ô∏è Calibre is not installed. Please install it first and make sure **calibre** and **ebook-meta** commands are available in your PATH.")
             return
-        
+
         is_ffmpeg_installed = check_if_ffmpeg_is_installed()
 
         if not is_ffmpeg_installed:
@@ -929,11 +974,11 @@ async def main():
             print(f"üìÇ Using book file: **{book_path}**")
 
         print("‚úÖ Book path set. Proceeding...\n")
-        
+
         # Early validation of the book file for M4B generation
         print("üîç Validating book file for M4B audiobook generation...")
         is_valid, error_message, metadata = validate_book_for_m4b_generation(book_path)
-        
+
         if not is_valid:
             print(f"‚ùå **Book validation failed**: {error_message}")
             print("\nüí° **Troubleshooting Tips:**")
@@ -943,7 +988,7 @@ async def main():
             print("   ‚Ä¢ Make sure the book file is not corrupted")
             print("   ‚Ä¢ Ensure the book file contains extractable metadata and cover image")
             return
-            
+
         print(f"‚úÖ **Book validation successful!**")
         print(f"   ‚Ä¢ Title: {metadata.get('Title', 'Unknown')}")
         print(f"   ‚Ä¢ Author: {metadata.get('Author(s)', 'Unknown')}")
@@ -959,7 +1004,7 @@ async def main():
         if(output_format not in ["aac", "m4a", "mp3", "wav", "opus", "flac", "pcm"]):
             print("\n‚ö†Ô∏è Invalid output format! Please choose from the give options")
             return
-        
+
     # Prompt user for narrator's gender selection
     print("\nüéôÔ∏è **Audiobook Narrator Voice Selection**")
     narrator_gender = input("üîπ Enter **male** if you want the book to be read in a male voice or **female** if you want the book to be read in a female voice: ").strip()
@@ -975,7 +1020,7 @@ async def main():
         print("üîπ Emotion tags add natural expressions like laughter, sighs, gasps to your audiobook")
         print("üîπ Available tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>")
         emotion_tags_option = input("üîπ Do you want to use emotion tags in the audiobook? Enter **yes** or **no**: ").strip().lower()
-        
+
         if emotion_tags_option in ["yes", "y", "true", "1"]:
             add_emotion_tags = True
             print("‚úÖ Emotion tags will be used in the audiobook!")
diff --git a/utils/add_emotion_tags_to_text.py b/utils/add_emotion_tags_to_text.py
index bed539e..626653b 100644
--- a/utils/add_emotion_tags_to_text.py
+++ b/utils/add_emotion_tags_to_text.py
@@ -16,9 +16,13 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import re
+import hashlib
+from datetime import datetime
 import json
 import os
 import asyncio
+import random
 import traceback
 from tqdm.asyncio import tqdm_asyncio # Use tqdm's async version for better updates
 from openai import AsyncOpenAI
@@ -39,33 +43,33 @@ model_name = OPENAI_MODEL_NAME
 def fix_orphaned_tags_and_punctuation(text, original_text):
     """
     Fix orphaned emotion tags and punctuation issues while preserving line count.
-    
+
     Args:
         text (str): Text with potential orphaned emotion tags
         original_text (str): Original text for reference
-        
+
     Returns:
         str: Fixed text with orphaned tags relocated and punctuation corrected
     """
     import re
-    
+
     lines = text.split('\n')
     original_lines = original_text.split('\n')
     fixed_lines = []
-    
+
     for i, line in enumerate(lines):
         fixed_line = line
-        
+
         # 1. Detect orphaned emotion tags (lines with mostly just a tag and minimal text)
         emotion_tags = re.findall(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>', line)
         if emotion_tags:
             # Remove tags to check remaining content
             line_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*', '', line).strip()
-            
+
             # If line has only tags or very minimal content (< 10 chars), it's likely orphaned
             if len(line_without_tags) < 10:
                 print(f"Detected potential orphaned tag in line: '{line}'")
-                
+
                 # Try to move tag to previous line if it looks like dialogue
                 if i > 0 and i-1 < len(fixed_lines):
                     prev_line = fixed_lines[i-1]
@@ -91,36 +95,36 @@ def fix_orphaned_tags_and_punctuation(text, original_text):
                     # Remove tags but keep the line content to preserve line count
                     fixed_line = line_without_tags if line_without_tags else original_lines[i] if i < len(original_lines) else ""
                     print(f"Removed orphaned tag, preserved line content")
-        
+
         # 2. Fix punctuation issues in dialogue with emotion tags
         if '"' in fixed_line and any(tag in fixed_line for tag in ['<laugh>', '<chuckle>', '<sigh>', '<cough>', '<sniffle>', '<groan>', '<yawn>', '<gasp>']):
             fixed_line = fix_dialogue_punctuation(fixed_line)
-        
+
         # Always append the line to preserve line count, even if it's empty
         fixed_lines.append(fixed_line)
-    
+
     return '\n'.join(fixed_lines)
 
 def move_tag_to_dialogue(dialogue_line, emotion_tag):
     """
     Move an emotion tag into dialogue, placing it before punctuation if present.
-    
+
     Args:
         dialogue_line (str): Line containing dialogue
         emotion_tag (str): Emotion tag to insert (e.g., '<laugh>')
-        
+
     Returns:
         str: Modified dialogue line with tag inserted
     """
     import re
-    
+
     # Find dialogue content within quotes
     dialogue_pattern = r'"([^"]*)"'
     match = re.search(dialogue_pattern, dialogue_line)
-    
+
     if match:
         dialogue_content = match.group(1)
-        
+
         # Check if dialogue ends with punctuation
         if dialogue_content and dialogue_content[-1] in '.!?,:;':
             # Insert tag before the punctuation
@@ -128,79 +132,79 @@ def move_tag_to_dialogue(dialogue_line, emotion_tag):
         else:
             # Add tag and punctuation at the end
             new_dialogue = dialogue_content + f" {emotion_tag}."
-        
+
         # Replace the dialogue content in the original line
         new_line = dialogue_line.replace(f'"{dialogue_content}"', f'"{new_dialogue}"')
         return new_line
-    
+
     return dialogue_line
 
 def fix_dialogue_punctuation(line):
     """
     Fix punctuation issues in dialogue lines with emotion tags.
-    
+
     Args:
         line (str): Line containing dialogue with emotion tags
-        
+
     Returns:
         str: Line with fixed punctuation
     """
     import re
-    
+
     # Pattern to find dialogue with emotion tags that might need punctuation fixes
     dialogue_pattern = r'"([^"]*<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>[^"]*)"'
-    
+
     def fix_match(match):
         dialogue_content = match.group(1)
-        
+
         # Check if the dialogue content ends with proper punctuation after the tag
         if not re.search(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*[.!?]', dialogue_content):
             # If tag is at the end without punctuation, add it
             if dialogue_content.endswith('>'):
                 dialogue_content += '.'
-        
+
         return f'"{dialogue_content}"'
-    
+
     return re.sub(dialogue_pattern, fix_match, line)
 
 def detect_remaining_orphaned_tags(text):
     """
     Detect any remaining orphaned emotion tags after attempted fixes.
-    
+
     Args:
         text (str): Text to check for orphaned tags
-        
+
     Returns:
         list: List of lines that still contain orphaned emotion tags
     """
     import re
-    
+
     orphaned_lines = []
     lines = text.split('\n')
-    
+
     for i, line in enumerate(lines):
         emotion_tags = re.findall(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>', line)
         if emotion_tags:
             # Remove tags to check remaining content
             line_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*', '', line).strip()
-            
+
             # If line has only tags or very minimal content (< 10 chars), it's still orphaned
             # But now we don't treat this as a fatal error since we preserve line count
             if len(line_without_tags) < 10:
                 print(f"Warning: Potential orphaned tag preserved at line {i+1}: '{line}'")
                 # Don't add to orphaned_lines since we're preserving these lines now
-    
+
     # Since we now preserve line count, we return empty list to indicate no fatal orphans
     return orphaned_lines
 
 def postprocess_emotion_tags(enhanced_text, original_text):
     """
     Comprehensive postprocessing to validate and clean emotion tags output.
-    
+
     Args:
         enhanced_text (str): Text returned by the LLM with emotion tags
         original_text (str): Original text segment before processing
-        
+
     Returns:
         dict: {
             'text': str,           # Validated text (cleaned or original)
@@ -211,21 +215,21 @@ def postprocess_emotion_tags(enhanced_text, original_text):
     """
     # Define allowed emotion tags (official Orpheus TTS supported tags)
     ALLOWED_TAGS = {'<laugh>', '<chuckle>', '<sigh>', '<cough>', '<sniffle>', '<groan>', '<yawn>', '<gasp>'}
-    
+
     try:
         # 1. Remove any backticks that the LLM might have added
         cleaned_text = enhanced_text.replace('`', '')
-        
+
         # 2. Check for invalid emotion tags
         import re
         found_tags = set(re.findall(r'<[^>]+>', cleaned_text))
         invalid_tags = found_tags - ALLOWED_TAGS
-        
+
         # Also check for common structural markup that might leak from prompts
-        structural_tags = {'<text_segment>', '</text_segment>', '<text>', '</text>', 
+        structural_tags = {'<text_segment>', '</text_segment>', '<text>', '</text>',
                           '<input>', '</input>', '<output>', '</output>'}
         found_structural = found_tags & structural_tags
-        
+
         if invalid_tags:
             if found_structural:
                 reason = f"Structural markup leaked from prompt: {found_structural}"
@@ -238,18 +242,18 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # 3. Check if text was significantly modified beyond adding tags
         # Remove all emotion tags to compare core content
         text_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>', '', cleaned_text)
-        
+
         # Remove ALL whitespace for content comparison - we only care about actual words/characters
         def remove_all_whitespace(text):
             return re.sub(r'\s+', '', text)
-        
+
         text_content_only = remove_all_whitespace(text_without_tags)
         original_content_only = remove_all_whitespace(original_text)
-        
+
         if text_content_only != original_content_only:
             reason = f"Text content was modified beyond adding tags."
             print(f"Warning: {reason}. Reverting to original text.")
@@ -259,13 +263,13 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # 4. Check if line breaks/newlines were preserved
         original_lines = original_text.split('\n')
         # Remove tags from enhanced text to check line structure
         enhanced_without_tags = re.sub(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*', '', cleaned_text)
         enhanced_lines = enhanced_without_tags.split('\n')
-        
+
         if len(original_lines) != len(enhanced_lines):
             reason = f"Line structure changed ({len(original_lines)} -> {len(enhanced_lines)} lines)"
             print(f"Warning: {reason}. Reverting to original text.")
@@ -275,9 +279,9 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # 5. Additional validation checks
-        
+
         # Check for malformed tags (missing < or >)
         malformed_tags = re.findall(r'<[^>]*$|^[^<]*>', cleaned_text)
         if malformed_tags:
@@ -289,7 +293,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # Check for HTML entities that shouldn't be there
         if '&lt;' in cleaned_text or '&gt;' in cleaned_text:
             reason = "HTML entities found in emotion tags"
@@ -300,7 +304,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # Check for tags breaking words (tag in the middle of a word)
         # Look for patterns like "wo<laugh>rd"
         word_breaking_pattern = r'\w<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\w'
@@ -313,7 +317,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # Check for excessive tag usage (more than 1 tag per 10 words as a safety check)
         word_count = len(original_text.split())
         tag_count = len(found_tags)
@@ -326,7 +330,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # Check for duplicate consecutive tags
         consecutive_tags = re.search(r'<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>\s*<(?:laugh|chuckle|sigh|cough|sniffle|groan|yawn|gasp)>', cleaned_text)
         if consecutive_tags:
@@ -338,13 +342,13 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # 6. Try to fix orphaned emotion tags and punctuation issues
         fixed_text = fix_orphaned_tags_and_punctuation(cleaned_text, original_text)
         if fixed_text != cleaned_text:
             print(f"Applied orphaned tag fixes")
             cleaned_text = fixed_text
-        
+
         # 7. Verify line count is preserved after fixing orphaned tags
         fixed_lines = cleaned_text.split('\n')
         if len(original_lines) != len(fixed_lines):
@@ -356,7 +360,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # 8. Final check for remaining orphaned tags after fixes
         remaining_orphans = detect_remaining_orphaned_tags(cleaned_text)
         if remaining_orphans:
@@ -368,7 +372,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
                 'reverted': True,
                 'reason': reason
             }
-        
+
         # All validations passed
         return {
             'text': cleaned_text,
@@ -376,7 +380,7 @@ def postprocess_emotion_tags(enhanced_text, original_text):
             'reverted': False,
             'reason': None
         }
-        
+
     except Exception as e:
         reason = f"Exception in postprocessing: {str(e)}"
         print(f"Error in postprocessing emotion tags: {e}")
@@ -387,6 +391,240 @@ def postprocess_emotion_tags(enhanced_text, original_text):
             'reason': reason
         }
 
+async def identify_entities(chapter_text, filtered_kb, retries=5, backoff_factor=2, base_delay=1):
+
+    # Create unique directory for this call
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
+    text_hash = hashlib.sha1(chapter_text.encode("utf-8")).hexdigest()[:8]
+    session_dir = os.path.join("temp_prompt_responses", f"{timestamp}_{text_hash}")
+    os.makedirs(session_dir, exist_ok=True)
+
+    combined_input = json.dumps({
+        "chapterText": chapter_text,
+        "knowledgeBase": filtered_kb
+    })
+
+    extraction_system_prompt  = (
+        "You are an advanced literary analysis assistant specialized in entity extraction. "
+        "Your goal is to identify all entities in a text, including characters, places, organizations, objects, powers, possessions, events, periods, or concepts. "
+        "Be extremely thorough and aim to identify at least 30 entities if possible. "
+        "However, exclude trivial, generic, or non-meaningful nouns such as 'air', 'rock', 'ants', 'dust', 'grass', or similar physical items unless they are symbolically or contextually important to the text. "
+        "Focus only on entities that are relevant, meaningful, or significant within the text's narrative or thematic context. "
+        "For each entity, provide detailed information including its type, historical/contextual information from a knowledge base, relevant relationships with other entities, and notable attributes or characteristics. "
+        "Always output strictly valid JSON with no extra commentary. "
+        "Use complete sentences for all descriptions, maintain consistent formatting, and include all detectable entities, even minor but meaningful ones."
+    )
+
+    extraction_user_prompt = f"""
+You are provided with the following chapter text and a knowledge base for reference:
+
+{combined_input}
+
+For this chapter, return a JSON object with the following structure:
+
+{{
+    "chapterNumber": <detected chapter number from text>,
+    "entities": {{
+        "<entity_name>": {{
+            "type": <entity type>,
+            "description": <detailed description including historical/contextual information>,
+        }},
+        ...
+    }}
+}}
+
+Do not include any text outside the JSON.
+"""
+
+    def validate_entities(entities_data):
+        """Validate that all entities contain required fields."""
+        if not isinstance(entities_data, dict):
+            raise ValueError("Entities data must be a dictionary.")
+
+        missing_fields = {}
+        for entity_name, entity_info in entities_data.items():
+            if not isinstance(entity_info, dict):
+                missing_fields[entity_name] = ["entity data not a dict"]
+                continue
+
+            missing = []
+            for field in ["type", "description"]:
+                if field not in entity_info or not entity_info[field]:
+                    missing.append(field)
+            if missing:
+                missing_fields[entity_name] = missing
+
+        if missing_fields:
+            raise ValueError(f"Entities missing required fields: {json.dumps(missing_fields, indent=2)}")
+
+
+    async def call_llm_with_retries(
+        system_prompt,
+        user_prompt,
+        retries,
+        backoff_factor,
+        base_delay,
+        label="",
+        session_dir=None,
+        validate_func=None
+    ):
+        """
+        Generic retry wrapper for LLM calls with full I/O logging and optional validation.
+
+        Args:
+            system_prompt (str): The system prompt sent to the LLM.
+            user_prompt (str): The user content sent to the LLM.
+            retries (int): Number of retry attempts.
+            backoff_factor (int): Multiplier for exponential backoff.
+            base_delay (int): Base delay in seconds.
+            label (str): Label for naming files (e.g., 'extraction' or 'refinement').
+            session_dir (str): Directory where logs are saved.
+            validate_func (callable, optional): Function that takes parsed output and raises on validation failure.
+        """
+        last_exception = None
+
+        for attempt in range(1, retries + 1):
+            try:
+                # ---- Save input ----
+                input_path = os.path.join(session_dir, f"{label}_attempt{attempt}_input.json")
+                with open(input_path, "w", encoding="utf-8") as f:
+                    json.dump({
+                        "system_prompt": system_prompt,
+                        "user_prompt": user_prompt
+                    }, f, ensure_ascii=False, indent=2)
+
+                # ---- LLM call ----
+                response = await openai_llm_client.chat.completions.create(
+                    model=model_name,
+                    messages=[
+                        {"role": "system", "content": system_prompt},
+                        {"role": "user", "content": user_prompt}
+                    ],
+                    temperature=0.2
+                )
+
+                raw_response = response.choices[0].message.content.strip()
+
+                # ---- Save raw response ----
+                raw_path = os.path.join(session_dir, f"{label}_attempt{attempt}_raw.txt")
+                with open(raw_path, "w", encoding="utf-8") as f:
+                    f.write(raw_response)
+
+
+                def clean_other_tags(raw_text):
+                    # Match JSON object from first { to last }
+                    match = re.search(r'(\{.*\})', raw_text, re.DOTALL)
+                    if match:
+                        return match.group(1)
+                    else:
+                        return raw_text  # fallback
+
+                # ---- Parse JSON ----
+                cleaned = clean_other_tags(clean_thinking_tags(raw_response))
+                if cleaned != raw_response:
+                    cleaned_path = os.path.join(session_dir, f"{label}_attempt{attempt}_cleaned.json")
+                    with open(cleaned_path, "w", encoding="utf-8") as f:
+                        json.dump(cleaned, f, ensure_ascii=False, indent=2)
+
+                parsed = json.loads(cleaned)
+
+                # ---- Save parsed output ----
+                parsed_path = os.path.join(session_dir, f"{label}_attempt{attempt}_parsed.json")
+                with open(parsed_path, "w", encoding="utf-8") as f:
+                    json.dump(parsed, f, ensure_ascii=False, indent=2)
+
+                # ---- Optional validation ----
+                if validate_func is not None:
+                    try:
+                        validate_func(parsed)
+                    except Exception as ve:
+                        # Log validation error and re-raise to trigger retry
+                        validation_log = os.path.join(session_dir, f"{label}_attempt{attempt}_validation_error.txt")
+                        with open(validation_log, "w", encoding="utf-8") as vf:
+                            vf.write(str(ve))
+                        raise ve  # trigger retry
+
+                return parsed, attempt - 1  # success ‚Üí parsed result and retries used
+
+            except Exception as e:
+                last_exception = e
+                print(f"Attempt {attempt}/{retries} failed: {e}")
+                traceback.print_exc()
+                if attempt < retries:
+                    delay = base_delay * (backoff_factor ** (attempt - 1)) + random.uniform(0, 0.5)
+                    print(f"Retrying in {delay:.1f}s...")
+                    await asyncio.sleep(delay)
+                else:
+                    raise last_exception
+
+     # ---- Call 1: Entity extraction ----
+    extraction_result, extraction_retries = await call_llm_with_retries(
+        extraction_system_prompt, extraction_user_prompt, retries, backoff_factor, base_delay, label="extraction", session_dir=session_dir
+    )
+
+    chapter_number = extraction_result.get("chapterNumber")
+    entities_data = extraction_result.get("entities", {})
+
+    # ----------- SECOND CALL: Entity Refinement -----------
+    refinement_system_prompt = (
+        "You are an expert in literary entity refinement. "
+        "You will receive a JSON object of extracted entities from a book. "
+        "Your task is to carefully review the list and remove any entities that are trivial, overly generic, "
+        "or non-specific to the book's fictional world. "
+        "Examples of entities to remove include common natural objects (like 'tree', 'stone', 'sky'), "
+        "generic materials ('cloth', 'metal'), or everyday nouns that are not symbolically or contextually significant. "
+        "Do not remove entities that play a thematic, symbolic, or narrative role. "
+        "Your goal is to preserve only meaningful, story-specific entities that add depth to the book's universe. "
+        "The final output must be valid JSON ‚Äî no commentary, explanations, or extra text outside the JSON."
+    )
+
+    refinement_user_prompt = f"""
+You are provided with a JSON object representing entities extracted from a book chapter.
+
+Review these entities and remove any that are trivial, generic, or unrelated to the specific world or context of the book.
+
+Keep only entities that are meaningful, unique, or relevant to the story, its characters, themes, or world-building.
+
+Here are the entities to refine:
+{json.dumps(entities_data, ensure_ascii=False, indent=2)}
+
+Do not add new entites not in the input above. You must only return valid JSON. Do not write any explanations, reasoning, or notes.
+Even if you remove an entity, do not comment on it. Only output the final JSON.
+Return the refined JSON in this exact structure:
+
+{{
+    "entities": {{
+        "<entity_name>": {{
+            "type": "<entity type>",
+            "description": "<detailed description including historical/contextual information>"
+        }},
+        ...
+    }}
+}}
+"""
+
+    refinement_result, refinement_retries = await call_llm_with_retries(
+        refinement_system_prompt,
+        refinement_user_prompt,
+        retries,
+        backoff_factor,
+        base_delay,
+        label="refinement",
+        session_dir=session_dir,
+        validate_func=lambda data: validate_entities(data.get("entities", data))
+    )
+
+    refined_entities = refinement_result.get("entities", refinement_result)
+    removed_entities = {name: info for name, info in entities_data.items() if name not in refined_entities}
+
+    removed_path = os.path.join(session_dir, "refinement_removed_entities.json")
+    with open(removed_path, "w", encoding="utf-8") as f:
+        json.dump(removed_entities, f, ensure_ascii=False, indent=2)
+
+    # ----------- FINAL OUTPUT -----------
+    total_retries = extraction_retries + refinement_retries
+    return chapter_number, refined_entities, total_retries
+
 # Consider adding @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
 async def enhance_text_with_emotions(text_segment):
     """Process a text segment, adding emotion tags."""
@@ -473,14 +711,14 @@ You are an expert editor specializing in preparing book scripts for Text-to-Spee
 
 **Line Break Preservation Examples:**
 
-* **Input:** 
+* **Input:**
 ```
 "Hello there," she said.
 
 He sighed deeply.
 "I don't know what to do."
 ```
-* **Good Output:** 
+* **Good Output:**
 ```
 "Hello there," she said.
 
@@ -488,18 +726,18 @@ He `<sigh>` sighed deeply.
 "I don't know what to do."
 ```
 
-* **Input:** 
+* **Input:**
 ```
 The room was quiet.
     "Are you okay?" she whispered.
-    
+
     There was no response.
 ```
-* **Good Output:** 
+* **Good Output:**
 ```
 The room was quiet.
     "Are you okay?" she whispered.
-    
+
     There was no response.
 ```
 
@@ -560,16 +798,16 @@ def create_chunks(text, chunk_size_lines=5):
 async def process_chunk_line_by_line(chunk):
     """
     Fallback function to process a chunk line by line when chunk-based processing fails.
-    
+
     Args:
         chunk (str): The text chunk that failed chunk-based processing
-        
+
     Returns:
         str: The chunk with emotion tags added line by line
     """
     lines = chunk.split('\n')
     processed_lines = []
-    
+
     for line in lines:
         if line.strip():  # Only process non-empty lines
             try:
@@ -582,7 +820,7 @@ async def process_chunk_line_by_line(chunk):
         else:
             # Preserve empty lines exactly
             processed_lines.append(line)
-    
+
     return '\n'.join(processed_lines)
 
 async def add_tags_to_text_chunks(text_to_process):
@@ -592,7 +830,7 @@ async def add_tags_to_text_chunks(text_to_process):
     lines = text_to_process.split('\n')
     non_empty_lines = [line for line in lines if line.strip()]  # Only keep non-empty lines
     filtered_text = '\n'.join(non_empty_lines)
-    
+
     yield f"Filtered text to match JSONL processing: {len(lines)} -> {len(non_empty_lines)} lines"
 
     # use a line-based chunker like the helper function above
@@ -610,7 +848,7 @@ async def add_tags_to_text_chunks(text_to_process):
             # Try chunk-based processing first
             enhancement_result = await enhance_text_with_emotions(chunk)
             final_result = enhancement_result['text']
-            
+
             # Check if postprocessing failed and we need fallback
             if enhancement_result['reverted'] and enhancement_result['reason']:
                 print(f"Chunk {chunk_index}: Chunk processing failed ({enhancement_result['reason']})")
@@ -620,7 +858,7 @@ async def add_tags_to_text_chunks(text_to_process):
                 except Exception as e:
                     print(f"Chunk {chunk_index}: Line-by-line fallback also failed: {e}")
                     final_result = chunk  # Use original chunk if both methods fail
-            
+
             progress_counter += 1
             return {
                 "index": chunk_index,
@@ -629,7 +867,7 @@ async def add_tags_to_text_chunks(text_to_process):
             }
 
     yield f"Processing {total_chunks} chunks for emotion tags (max {LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE} concurrent)..."
-    
+
     # Create tasks with chunk indices
     tasks = []
     for i, chunk in enumerate(chunks):
@@ -637,17 +875,17 @@ async def add_tags_to_text_chunks(text_to_process):
 
     # Process tasks and yield progress updates as they complete
     results = [None] * total_chunks  # Pre-allocate to maintain order
-    
+
     for completed_task in asyncio.as_completed(tasks):
         task_result = await completed_task
         chunk_index = task_result["index"]
         chunk_result = task_result["result"]
         current_progress = task_result["progress"]
-        
+
         results[chunk_index] = chunk_result
-        
+
         yield f"Processed {current_progress}/{total_chunks} emotion tag chunks..."
-    
+
     yield f"Completed processing all {total_chunks} emotion tag chunks"
 
     # Reassemble the book, respecting the original chunk separation
@@ -655,7 +893,7 @@ async def add_tags_to_text_chunks(text_to_process):
 
     # Validate that line count is preserved (using filtered text for comparison)
     enhanced_lines = enhanced_text.split('\n')
-    
+
     if len(non_empty_lines) != len(enhanced_lines):
         error_msg = f"ERROR: Line count mismatch after emotion processing! Filtered: {len(non_empty_lines)}, Enhanced: {len(enhanced_lines)}"
         yield error_msg
@@ -676,38 +914,38 @@ async def add_tags_to_text_chunks(text_to_process):
 async def process_emotion_tags_for_jsonl_data(json_data_array):
     """
     Efficiently process emotion tags for JSONL data using optimized chunk processing.
-    
+
     This function extracts all text from the JSONL data, processes it using the optimized
     add_tags_to_text_chunks function, then maps the enhanced text back to the original
     JSONL structure while preserving speaker attributions.
-    
+
     Args:
         json_data_array (list): Array of JSONL objects with 'line' and 'speaker' fields
-        
+
     Yields:
         str: Progress updates during emotion tags processing
-        
+
     Returns:
         list: Updated JSONL array with emotion tags added to the text
     """
     # Extract all lines of text while preserving the mapping to original entries
     all_text_lines = []
     line_mappings = []  # Maps processed text lines back to original jsonl entries
-    
+
     for i, item in enumerate(json_data_array):
         if "line" in item and item["line"] and item["line"].strip():
             all_text_lines.append(item["line"])
             line_mappings.append(i)  # Store the index in the original array
-    
+
     if not all_text_lines:
         yield "No text found to process for emotion tags"
         yield json_data_array  # No text to process
-    
+
     yield f"Preparing {len(all_text_lines)} lines for emotion tags processing..."
-    
+
     # Combine all text into a single block for optimized chunk processing
     combined_text = "\n".join(all_text_lines)
-    
+
     # Use the optimized add_tags_to_text_chunks function and yield its progress
 
     async for progress in add_tags_to_text_chunks(combined_text):
@@ -716,20 +954,20 @@ async def process_emotion_tags_for_jsonl_data(json_data_array):
     enhanced_text = None
     with open("tag_added_lines_chunks.txt", "r") as f:
         enhanced_text = f.read()
-    
+
     # Split the enhanced text back into individual lines
     enhanced_lines = enhanced_text.split("\n")
-    
+
     yield f"Mapping enhanced text back to {len(line_mappings)} original entries..."
-    
+
     # Map the enhanced lines back to the original JSONL structure
     # Handle potential line count mismatches gracefully
     for i, enhanced_line in enumerate(enhanced_lines):
         if i < len(line_mappings):
             original_index = line_mappings[i]
             json_data_array[original_index]["line"] = enhanced_line
-    
+
     yield f"Successfully enhanced {len(line_mappings)} lines with emotion tags"
-    
+
     # Final yield with the result
     yield json_data_array
\ No newline at end of file
diff --git a/utils/audiobook_utils.py b/utils/audiobook_utils.py
index df8ac0b..e7f9abd 100644
--- a/utils/audiobook_utils.py
+++ b/utils/audiobook_utils.py
@@ -16,6 +16,8 @@ You should have received a copy of the GNU General Public License
 along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+import tempfile
+import shutil
 import subprocess
 import re
 import os
@@ -36,20 +38,20 @@ def escape_metadata(value):
 def validate_file_path(file_path):
     """
     Validates that a file path is safe using allowlist approach.
-    
+
     Args:
         file_path (str): The file path to validate
-        
+
     Returns:
         bool: True if path is safe, False otherwise
     """
     if not file_path or not isinstance(file_path, str):
         return False
-    
+
     # Use allowlist-based validation
     if not validate_file_path_allowlist(file_path):
         return False
-        
+
     # Check if file exists and is readable
     try:
         return os.path.exists(file_path) and os.access(file_path, os.R_OK)
@@ -69,14 +71,14 @@ def get_ebook_metadata_with_cover(book_path):
     # Validate file path
     if not validate_file_path(book_path):
         raise ValueError(f"Invalid or unsafe book path: {book_path}")
-        
+
     # Get ebook-meta binary path securely
     allowed_commands = ['which', 'ebook-meta']
     ebook_meta_bin_result = run_shell_command_secure("which ebook-meta", allowed_commands)
-    
+
     if not ebook_meta_bin_result or not ebook_meta_bin_result.stdout.strip():
         raise RuntimeError("ebook-meta command not found")
-        
+
     ebook_meta_bin_path = ebook_meta_bin_result.stdout.strip()
 
     # Build secure command as list
@@ -85,7 +87,7 @@ def get_ebook_metadata_with_cover(book_path):
     # Run the command securely using our centralized function
     allowed_ebook_commands = ['ebook-meta']
     result = run_shell_command_secure(command, allowed_ebook_commands)
-    
+
     if not result:
         raise RuntimeError("Failed to extract metadata")
 
@@ -95,9 +97,9 @@ def get_ebook_metadata_with_cover(book_path):
         if ": " in line:
             key, value = line.split(": ", 1)
             metadata[key.strip()] = value.strip()
-    
+
     return metadata
-    
+
 def get_audio_duration_using_ffprobe(file_path):
     """
     Returns the duration of an audio file in milliseconds using ffprobe.
@@ -111,7 +113,7 @@ def get_audio_duration_using_ffprobe(file_path):
     # Validate file path
     if not validate_file_path(file_path):
         raise ValueError(f"Invalid or unsafe file path: {file_path}")
-        
+
     # Construct the command to execute
     cmd = [
         "ffprobe",  # Use ffprobe to get the duration
@@ -122,14 +124,14 @@ def get_audio_duration_using_ffprobe(file_path):
         "default=noprint_wrappers=1:nokey=1",  # Print the duration without any additional information
         file_path  # Specify the file to analyze
     ]
-    
+
     # Run the command securely
     allowed_commands = ['ffprobe']
     result = run_shell_command_secure(cmd, allowed_commands)
-    
+
     if not result or result.returncode != 0:
         raise RuntimeError(f"Failed to get audio duration: {result.stderr if result else 'Unknown error'}")
-        
+
     # Convert the output to an integer (in milliseconds) and return it
     return int(float(result.stdout.strip()) * 1000)
 
@@ -138,17 +140,17 @@ def get_audio_duration_using_raw_ffmpeg(file_path):
     # Validate file path
     if not validate_file_path(file_path):
         raise ValueError(f"Invalid or unsafe file path: {file_path}")
-        
+
     cmd = ["ffmpeg", "-y", "-i", file_path, "-f", "null", "-"]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result:
             return None
-            
+
         stderr_output = result.stderr
 
         # Look for the final timestamp (time=xx:xx:xx.xx) in FFmpeg output
@@ -180,14 +182,14 @@ def generate_chapters_file(chapter_files, output_file="chapters.txt"):
         for chapter in chapter_files:
             duration = get_audio_duration_using_ffprobe(os.path.join("temp_audio", chapter))
             end_time = start_time + duration
-            
+
             # Write the chapter metadata to the file
             f.write("[CHAPTER]\n")
             f.write("TIMEBASE=1/1000\n")
             f.write(f"START={start_time}\n")
             f.write(f"END={end_time}\n")
             f.write(f"title={os.path.splitext(chapter)[0]}\n\n")  # Use filename as chapter title
-            
+
             # Update the start time for the next chapter
             start_time = end_time
 
@@ -195,24 +197,24 @@ def create_m4a_file_from_raw_aac_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     cmd = ["ffmpeg", "-y", "-i", input_file_path, "-c", "copy", output_file_path]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating M4A from AAC: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_m4a_file_from_wav_file(input_file_path, output_file_path):
     """
     Convert WAV to M4A using AAC encoding with intelligent quality settings.
@@ -221,28 +223,28 @@ def create_m4a_file_from_wav_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file to preserve them
     audio_props = get_audio_properties(input_file_path)
     sample_rate = audio_props["sample_rate"]
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-c:a", "aac", "-b:a", "256k",  # High quality AAC
         "-ar", str(sample_rate),  # Preserve original sample rate
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating M4A from WAV: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
@@ -255,24 +257,24 @@ def create_aac_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     cmd = ["ffmpeg", "-y", "-i", input_file_path, "-c", "copy", output_file_path]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating AAC from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_mp3_file_from_m4a_file(input_file_path, output_file_path):
     """
     Convert M4A to MP3 preserving original sample rate and quality.
@@ -281,37 +283,37 @@ def create_mp3_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file
     audio_props = get_audio_properties(input_file_path)
     sample_rate = audio_props["sample_rate"]
-    
+
     # Choose appropriate bitrate based on sample rate and content type (speech)
     bitrate = "192k" if sample_rate >= 44100 else "160k"
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-c:a", "libmp3lame", "-b:a", bitrate,
         "-ar", str(sample_rate),  # Preserve original sample rate
         "-q:a", "2",  # High quality VBR setting
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating MP3 from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_wav_file_from_m4a_file(input_file_path, output_file_path):
     """
     Convert M4A to WAV preserving original sample rate and channel layout.
@@ -320,34 +322,34 @@ def create_wav_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file to preserve them
     audio_props = get_audio_properties(input_file_path)
     sample_rate = audio_props["sample_rate"]
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-c:a", "pcm_s16le",  # 16-bit PCM (standard WAV)
         "-ar", str(sample_rate),  # Preserve original sample rate
         # Note: Not specifying channel count to preserve original layout
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating WAV from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_opus_file_from_m4a_file(input_file_path, output_file_path):
     """
     Convert M4A to Opus with intelligent quality settings.
@@ -356,11 +358,11 @@ def create_opus_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file
     audio_props = get_audio_properties(input_file_path)
     original_sample_rate = audio_props["sample_rate"]
-    
+
     # Opus supported sample rates: 8000, 12000, 16000, 24000, 48000
     # Map input sample rates to closest supported Opus rate
     if original_sample_rate <= 10000:
@@ -378,29 +380,29 @@ def create_opus_file_from_m4a_file(input_file_path, output_file_path):
     else:
         opus_sample_rate = 48000  # For 44100Hz and higher
         bitrate = "160k"
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-c:a", "libopus", "-b:a", bitrate,
         "-ar", str(opus_sample_rate),  # Use compatible Opus sample rate
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating Opus from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_flac_file_from_m4a_file(input_file_path, output_file_path):
     """
     Convert M4A to FLAC (lossless compression).
@@ -409,34 +411,34 @@ def create_flac_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file to preserve them
     audio_props = get_audio_properties(input_file_path)
     sample_rate = audio_props["sample_rate"]
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-c:a", "flac",
         "-ar", str(sample_rate),  # Preserve original sample rate
         # FLAC is lossless so we preserve everything
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating FLAC from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def create_pcm_file_from_m4a_file(input_file_path, output_file_path):
     """
     Convert M4A to raw PCM preserving original audio properties.
@@ -445,36 +447,36 @@ def create_pcm_file_from_m4a_file(input_file_path, output_file_path):
     # Validate file paths
     if not validate_file_path(input_file_path):
         raise ValueError(f"Invalid input file path: {input_file_path}")
-        
+
     # Get properties of input file to preserve them
     audio_props = get_audio_properties(input_file_path)
     sample_rate = audio_props["sample_rate"]
     channels = audio_props["channels"]
-    
+
     cmd = [
-        "ffmpeg", "-y", "-i", input_file_path, 
+        "ffmpeg", "-y", "-i", input_file_path,
         "-f", "s16le",  # 16-bit little-endian format
-        "-acodec", "pcm_s16le", 
+        "-acodec", "pcm_s16le",
         "-ar", str(sample_rate),  # Preserve original sample rate
         "-ac", str(channels),  # Preserve original channel count
         output_file_path
     ]
-    
+
     try:
         # Run the command securely
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             print(f"Error creating PCM from M4A: {error_msg}")
             return None
-            
+
     except Exception as e:
         print(f"Error: {e}")
         traceback.print_exc()
         return None
-    
+
 def convert_audio_file_formats(input_format, output_format, folder_path, file_name):
         input_path = os.path.join(folder_path, f"{file_name}.{input_format}")
         output_path = os.path.join(folder_path, f"{file_name}.{output_format}")
@@ -498,7 +500,7 @@ def convert_audio_file_formats(input_format, output_format, folder_path, file_na
             create_flac_file_from_m4a_file(input_path, output_path)
         elif output_format == "pcm":
             create_pcm_file_from_m4a_file(input_path, output_path)
-    
+
 def merge_chapters_to_m4b(book_path, chapter_files):
     """
     Uses ffmpeg to merge all chapter files into an M4B audiobook.
@@ -512,9 +514,9 @@ def merge_chapters_to_m4b(book_path, chapter_files):
     # Validate inputs
     if not validate_file_path(book_path):
         raise ValueError(f"Invalid or unsafe book path: {book_path}")
-        
+
     file_list_path = "chapter_list.txt"
-    
+
     with open(file_list_path, "w", encoding='utf-8') as f:
         for chapter in chapter_files:
             # Validate each chapter file path
@@ -530,7 +532,7 @@ def merge_chapters_to_m4b(book_path, chapter_files):
     languages = escape_metadata(metadata.get("Languages", ""))
     published_date = escape_metadata(metadata.get("Published", ""))
     comments = escape_metadata(metadata.get("Comments", ""))
-    
+
     # Generate chapter metadata
     generate_chapters_file(chapter_files, "chapters.txt")
 
@@ -544,17 +546,17 @@ def merge_chapters_to_m4b(book_path, chapter_files):
 
     # Build secure FFmpeg command as list
     ffmpeg_cmd = [
-        "ffmpeg", "-y", 
+        "ffmpeg", "-y",
         "-f", "concat", "-safe", "0", "-i", file_list_path,
-        "-i", cover_image, 
+        "-i", cover_image,
         "-i", "chapters.txt",
-        "-c", "copy", 
-        "-map", "0", 
-        "-map", "1", 
-        "-disposition:v:0", "attached_pic", 
+        "-c", "copy",
+        "-map", "0",
+        "-map", "1",
+        "-disposition:v:0", "attached_pic",
         "-map_metadata", "2"
     ]
-    
+
     # Add metadata safely
     if title:
         ffmpeg_cmd.extend(["-metadata", f"title={title}"])
@@ -571,17 +573,17 @@ def merge_chapters_to_m4b(book_path, chapter_files):
         ffmpeg_cmd.extend(["-metadata", f"date={published_date}"])
     if comments:
         ffmpeg_cmd.extend(["-metadata", f"description={comments}"])
-        
+
     ffmpeg_cmd.append(output_m4b)
-    
+
     # Use centralized secure command execution
     allowed_ffmpeg_commands = ['ffmpeg']
     result = run_shell_command_secure(ffmpeg_cmd, allowed_ffmpeg_commands)
-    
+
     if not result or result.returncode != 0:
         error_msg = result.stderr if result else "Unknown error"
         raise RuntimeError(f"FFmpeg failed: {error_msg}")
-        
+
     print(f"Audiobook created: {output_m4b}")
 
 def add_silence_to_audio_file_by_appending_pre_generated_silence(temp_dir, input_file_name):
@@ -602,31 +604,31 @@ def add_silence_to_audio_file_by_reencoding_using_ffmpeg(temp_dir, input_file_na
     # Validate inputs
     if not validate_file_path(temp_dir) or not input_file_name:
         raise ValueError("Invalid temporary directory or file name")
-        
+
     # Validate pause_duration format (basic check)
     if not re.match(r'^\d{2}:\d{2}:\d{2}$', pause_duration):
         raise ValueError("Invalid pause duration format. Expected HH:MM:SS")
-    
+
     silence_path = os.path.join(temp_dir, "silence.aac")
     input_path = os.path.join(temp_dir, input_file_name)
-    
+
     # Validate file paths
     if not validate_file_path(input_path):
         raise ValueError(f"Invalid input file path: {input_path}")
-    
+
     # Generate a silence file with the specified duration - secure command
     generate_silence_command = [
-        "ffmpeg", "-y", 
-        "-f", "lavfi", "-i", "anullsrc=r=44100:cl=mono", 
-        "-t", pause_duration, 
-        "-c:a", "aac", 
+        "ffmpeg", "-y",
+        "-f", "lavfi", "-i", "anullsrc=r=44100:cl=mono",
+        "-t", pause_duration,
+        "-c:a", "aac",
         silence_path
     ]
-    
+
     # Use centralized secure command execution
     allowed_ffmpeg_commands = ['ffmpeg']
     result = run_shell_command_secure(generate_silence_command, allowed_ffmpeg_commands)
-    
+
     if not result or result.returncode != 0:
         error_msg = result.stderr if result else "Unknown error"
         raise RuntimeError(f"Failed to generate silence: {error_msg}")
@@ -634,17 +636,17 @@ def add_silence_to_audio_file_by_reencoding_using_ffmpeg(temp_dir, input_file_na
     # Add the silence to the end of the audio file - secure command
     temp_output_path = os.path.join(temp_dir, "temp_audio_file.aac")
     add_silence_command = [
-        "ffmpeg", "-y", 
-        "-i", input_path, 
-        "-i", silence_path, 
-        "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[out]", 
-        "-map", "[out]", 
+        "ffmpeg", "-y",
+        "-i", input_path,
+        "-i", silence_path,
+        "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[out]",
+        "-map", "[out]",
         temp_output_path
     ]
-    
+
     # Use centralized secure command execution
     result = run_shell_command_secure(add_silence_command, allowed_ffmpeg_commands)
-    
+
     if not result or result.returncode != 0:
         error_msg = result.stderr if result else "Unknown error"
         raise RuntimeError(f"Failed to add silence: {error_msg}")
@@ -655,59 +657,175 @@ def add_silence_to_audio_file_by_reencoding_using_ffmpeg(temp_dir, input_file_na
     except OSError as e:
         raise RuntimeError(f"Failed to rename file: {e}")
 
-def merge_chapters_to_standard_audio_file(chapter_files):
+def generate_line_timings_json(line_timings, entities):
     """
-    Uses ffmpeg to merge all chapter files into a standard M4A audio file).
+    Convert line timing data to a compact JSON string suitable for embedding
+    in FFmpeg metadata (comment field).
 
-    This function takes a list of chapter files and an output format as input, and generates a standard M4A audio file.
+    Escapes special characters per FFMETADATA spec:
+        '=', ';', '#', '\\', and newline.
+    """
+    if not line_timings:
+        return None
+
+    def escape_ffmetadata_chars(text: str) -> str:
+        text = text.replace("\\", "\\\\")
+        text = text.replace("=", "\\=")
+        text = text.replace(";", "\\;")
+        text = text.replace("#", "\\#")
+        return text
+
+    output = {
+        "entities": entities,
+        "lines": [
+            {
+                "startTime": round(line["start"], 3),
+                "endTime": round(line["end"], 3),
+                "text": line["text"]
+            }
+            for line in line_timings
+        ]
+    }
+
+    json_comment = json.dumps(output, ensure_ascii=False)
+    json_comment = escape_ffmetadata_chars(json_comment)
+
+    return json_comment
+
+def merge_chapters_to_standard_audio_file(chapter_files, line_timings=None, chapter_to_entities=None):
+    """
+    Merge chapter audio files into a standard M4A with jumpable chapters.
+    Converts WAV/PCM/AAC chapters to AAC/M4A before concatenation.
+    Uses original filenames (without extension) as chapter titles.
 
     Args:
-        chapter_files (list): A list of the paths to the individual chapter audio files.
+        chapter_files (list): A list of the names of individual chapter audio files (inside temp_audio).
     """
-    file_list_path = "chapter_list.txt"
-    
-    # Write the list of chapter files to a text file (ffmpeg input)
-    with open(file_list_path, "w", encoding='utf-8') as f:
-        for chapter in chapter_files:
-            chapter_path = os.path.join('temp_audio', chapter)
-            # Validate each chapter file
-            if not validate_file_path(chapter_path):
-                raise ValueError(f"Invalid chapter file: {chapter}")
-            f.write(f"file '{chapter_path}'\n")
+    if not chapter_files:
+        raise ValueError("No chapter files provided.")
 
-    # Construct the output file path
-    output_file = "generated_audiobooks/audiobook.m4a"
+    os.makedirs("generated_audiobooks", exist_ok=True)
 
-    # Validate file list exists
-    if not validate_file_path(file_list_path):
-        raise ValueError("Chapter list file is invalid")
+    # Temporary folder for converted M4A chapters
+    temp_dir = tempfile.mkdtemp(prefix="chapters_")
+    converted_files = []
 
-    # Construct secure ffmpeg command
-    ffmpeg_cmd = [
-        "ffmpeg", "-y", 
-        "-f", "concat", "-safe", "0", 
-        "-i", file_list_path, 
-        "-c", "copy", 
+    # 1. Convert all chapters to M4A (if needed)
+    for chapter in chapter_files:
+        input_path = os.path.join("temp_audio", chapter)
+        if not os.path.exists(input_path):
+            raise ValueError(f"Invalid chapter file: {chapter}")
+
+        output_path = os.path.join(temp_dir, os.path.splitext(chapter)[0] + ".m4a")
+
+        if chapter.lower().endswith(".wav"):
+            create_m4a_file_from_wav_file(input_path, output_path)
+        elif chapter.lower().endswith(".aac"):
+            create_m4a_file_from_raw_aac_file(input_path, output_path)
+        elif chapter.lower().endswith(".m4a"):
+            shutil.copy(input_path, output_path)
+        else:
+            raise ValueError(f"Unsupported chapter format: {chapter}")
+
+        converted_files.append(output_path)
+
+    # 2. Create concat list
+    concat_file = os.path.join(temp_dir, "chapter_list.txt")
+    with open(concat_file, "w", encoding="utf-8") as f:
+        for path in converted_files:
+            f.write(f"file '{path}'\n")
+
+    # 3. Merge into one M4A
+    temp_merged = os.path.join(temp_dir, "temp_merged.m4a")
+    subprocess.run([
+        "ffmpeg", "-y",
+        "-f", "concat", "-safe", "0",
+        "-i", concat_file,
+        "-c", "copy",
+        temp_merged
+    ], check=True)
+
+    # 4. Create chapter metadata
+    metadata_file = os.path.join("generated_audiobooks/", "metadata.txt")
+    with open(metadata_file, "w", encoding="utf-8") as f:
+        start_time = 0
+        chapter_metadata = []
+        output_entities = []
+        if chapter_to_entities:
+            output_entities.append({
+                "timestamp": 0,
+                "snapshot": chapter_to_entities["__original_kb__"]
+            })
+        for chapter, path in zip(chapter_files, converted_files):
+            # Get duration in seconds
+            result = subprocess.run(
+                ["ffprobe", "-v", "error", "-show_entries",
+                 "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", path],
+                capture_output=True, text=True, check=True
+            )
+            duration = float(result.stdout.strip())
+            end_time = start_time + int(duration * 1000)
+
+            # Use original filename (without extension) as chapter title
+            chapter_title = os.path.splitext(os.path.basename(chapter))[0]
+            if chapter_to_entities:
+                entities = chapter_to_entities[chapter_title]
+                output_entities.append({
+                    'timestamp': int(end_time/1000),
+                    "snapshot": entities
+                })
+            chapter_metadata.append(f"[CHAPTER]\nTIMEBASE=1/1000\nSTART={start_time}\nEND={end_time}\ntitle={chapter_title}\n")
+
+            start_time = end_time
+
+        f.write(";FFMETADATA1\n")
+
+        json_comment = generate_line_timings_json(line_timings, output_entities)
+        if json_comment:
+            print("Embedding line timing information as JSON in metadata comment")
+            f.write(f"comment={json_comment}\n\n")
+
+        for line in chapter_metadata:
+            f.write(line)
+
+    # 5. Attach metadata
+    output_file = "generated_audiobooks/audiobook.m4a"
+    subprocess.run([
+        "ffmpeg", "-y",
+        "-i", temp_merged,
+        "-i", metadata_file,
+        "-map_metadata", "1",
+        "-c", "copy",
         output_file
-    ]
+    ], check=True)
+
+    # 6. Clean up
+    shutil.rmtree(temp_dir)
+
+    print(f"Audiobook created with chapters: {output_file}")
 
-    # Use centralized secure command execution
-    allowed_ffmpeg_commands = ['ffmpeg']
-    result = run_shell_command_secure(ffmpeg_cmd, allowed_ffmpeg_commands)
-    
-    if not result or result.returncode != 0:
-        error_msg = result.stderr if result else "Unknown error"
-        raise RuntimeError(f"FFmpeg failed: {error_msg}")
-        
-    print(f"Audiobook created: {output_file}")
 
-def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir, temp_audio_dir):
+def get_duration(path):
+    result = subprocess.run(
+            [
+                "ffprobe", "-v", "error",
+                "-show_entries", "format=duration",
+                "-of", "default=noprint_wrappers=1:nokey=1",
+                path
+            ],
+            capture_output=True,
+            text=True,
+            check=True
+        )
+    return float(result.stdout.strip())
+
+def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir, temp_audio_dir, line_text, timings = [], silence_duration_ms=0):
     """
     Memory-efficient chapter assembly using FFmpeg instead of PyDub.
-    
+
     This function concatenates line audio files into a chapter using FFmpeg's concat filter,
     which is much more memory-efficient than loading all audio into PyDub AudioSegments.
-    
+
     Args:
         chapter_file (str): The name of the chapter file to create
         line_indices (list): List of line indices that belong to this chapter
@@ -716,10 +834,13 @@ def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir
     """
     if not line_indices:
         return
-    
+
     # Create a temporary file list for FFmpeg concat
     file_list_path = os.path.join(temp_audio_dir, f"chapter_list_{chapter_file}.txt")
-    
+    if timings:
+        current_time = timings[-1]['end']
+    else:
+        current_time = 0.0
     try:
         with open(file_list_path, "w", encoding='utf-8') as f:
             for line_index in sorted(line_indices):
@@ -727,24 +848,34 @@ def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir
                 # Use absolute path to avoid issues with FFmpeg concat
                 abs_line_path = os.path.abspath(line_path)
                 f.write(f"file '{abs_line_path}'\n")
-        
+                start_time = current_time
+                current_time =  start_time + get_duration(abs_line_path)
+                timings.append({
+                    "text": line_text[line_index],
+                    "start": start_time,
+                    "end": current_time
+                })
+
+            if line_indices:
+                timings[-1]['end'] += (silence_duration_ms / 1000.0)
+
         # Create the full chapter file path
         chapter_path = os.path.join(temp_audio_dir, chapter_file)
-        
+
         # Use FFmpeg to concatenate the audio files
         ffmpeg_cmd = [
-            "ffmpeg", "-y", "-f", "concat", "-safe", "0", 
+            "ffmpeg", "-y", "-f", "concat", "-safe", "0",
             "-i", file_list_path, "-c", "copy", chapter_path
         ]
-        
+
         # Use centralized secure command execution
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(ffmpeg_cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             raise RuntimeError(f"FFmpeg chapter assembly failed: {error_msg}")
-        
+
     finally:
         # Clean up the temporary file list
         if os.path.exists(file_list_path):
@@ -753,17 +884,17 @@ def assemble_chapter_with_ffmpeg(chapter_file, line_indices, temp_line_audio_dir
 def get_audio_properties(file_path):
     """
     Extract audio properties from an audio file using ffprobe.
-    
+
     This function analyzes an audio file and returns its sample rate, channel count,
     and channel layout, which is useful for ensuring compatibility when processing audio.
-    
+
     Args:
         file_path (str): Path to the audio file to analyze
-        
+
     Returns:
         dict: Dictionary containing:
             - sample_rate (int): Sample rate in Hz
-            - channels (int): Number of audio channels  
+            - channels (int): Number of audio channels
             - channel_layout (str): Channel layout ("mono" or "stereo")
             - duration (float): Duration in seconds (bonus info)
     """
@@ -777,33 +908,33 @@ def get_audio_properties(file_path):
             "channel_layout": "mono",
             "duration": 0.0
         }
-    
+
     # Get audio properties of the file
     probe_cmd = [
-        "ffprobe", "-v", "quiet", "-print_format", "json", 
+        "ffprobe", "-v", "quiet", "-print_format", "json",
         "-show_streams", "-show_format", file_path
     ]
-    
+
     try:
         # Use centralized secure command execution
         allowed_commands = ['ffprobe']
         probe_result = run_shell_command_secure(probe_cmd, allowed_commands)
-        
+
         if not probe_result or probe_result.returncode != 0:
             raise ValueError("ffprobe failed")
-            
+
         probe_data = json.loads(probe_result.stdout)
-        
+
         # Extract audio stream properties
         audio_stream = next((s for s in probe_data["streams"] if s["codec_type"] == "audio"), None)
         if audio_stream:
             sample_rate = int(audio_stream["sample_rate"])
             channels = int(audio_stream["channels"])
             channel_layout = "mono" if channels == 1 else "stereo"
-            
+
             # Get duration from format info
             duration = float(probe_data["format"]["duration"]) if "format" in probe_data and "duration" in probe_data["format"] else 0.0
-            
+
             return {
                 "sample_rate": sample_rate,
                 "channels": channels,
@@ -818,7 +949,7 @@ def get_audio_properties(file_path):
                 "channel_layout": "mono",
                 "duration": 0.0
             }
-            
+
     except (ValueError, json.JSONDecodeError, KeyError):
         # Fallback to common values if probe fails
         return {
@@ -831,10 +962,10 @@ def get_audio_properties(file_path):
 def add_silence_to_chapter_with_ffmpeg(chapter_path, silence_duration_ms=1000):
     """
     Memory-efficient silence addition using FFmpeg instead of PyDub.
-    
+
     This function adds silence to the end of a chapter file using FFmpeg,
     which is much more memory-efficient than loading the entire chapter into PyDub.
-    
+
     Args:
         chapter_path (str): Path to the chapter audio file
         silence_duration_ms (int): Duration of silence to add in milliseconds (default: 1000ms = 1 second)
@@ -842,51 +973,51 @@ def add_silence_to_chapter_with_ffmpeg(chapter_path, silence_duration_ms=1000):
     # Validate chapter path
     if not validate_file_path(chapter_path):
         raise ValueError(f"Invalid chapter path: {chapter_path}")
-        
+
     # Get audio properties of the chapter file
     audio_props = get_audio_properties(chapter_path)
     sample_rate = audio_props["sample_rate"]
     channel_layout = audio_props["channel_layout"]
-    
+
     # Use sample-rate specific silence file
     silence_file_path = f"static_files/silence_{sample_rate}hz.wav"
-    
+
     # Check if silence file exists, if not create it
     if not os.path.exists(silence_file_path):
         generate_silence_file(silence_file_path, silence_duration_ms, sample_rate, channel_layout)
-    
+
     # Create temporary output path with proper extension
     temp_output_path = f"{chapter_path}.temp.wav"
-    
+
     # Create temporary concat list file
     concat_list_path = f"{chapter_path}.concat_list.txt"
-    
+
     try:
         # Create a concat list file for FFmpeg
         with open(concat_list_path, "w", encoding='utf-8') as f:
             f.write(f"file '{os.path.abspath(chapter_path)}'\n")
             f.write(f"file '{os.path.abspath(silence_file_path)}'\n")
-        
+
         # Use FFmpeg concat demuxer to join the files with lossless copy
         ffmpeg_cmd = [
-            "ffmpeg", "-y", 
+            "ffmpeg", "-y",
             "-f", "concat", "-safe", "0",
             "-i", concat_list_path,
             "-c", "copy",  # LOSSLESS: Copy without re-encoding
             temp_output_path
         ]
-        
+
         # Use centralized secure command execution
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(ffmpeg_cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             raise RuntimeError(f"FFmpeg silence addition failed: {error_msg}")
-        
+
         # Replace the original file with the new one
         os.replace(temp_output_path, chapter_path)
-        
+
     except Exception as e:
         # Clean up temp files if FFmpeg failed
         if os.path.exists(temp_output_path):
@@ -901,10 +1032,10 @@ def add_silence_to_chapter_with_ffmpeg(chapter_path, silence_duration_ms=1000):
 def generate_silence_file(output_path, duration_ms=1000, sample_rate=44100, channel_layout="mono"):
     """
     Generate a silence audio file using FFmpeg with specific audio properties.
-    
+
     This function creates a WAV file containing silence that matches the sample rate
     and channel layout of the TTS-generated audio for perfect compatibility.
-    
+
     Args:
         output_path (str): Path where the silence file will be saved
         duration_ms (int): Duration of silence in milliseconds (default: 1000ms = 1 second)
@@ -913,30 +1044,30 @@ def generate_silence_file(output_path, duration_ms=1000, sample_rate=44100, chan
     """
     # Ensure the directory exists
     os.makedirs(os.path.dirname(output_path), exist_ok=True)
-    
+
     # Convert milliseconds to seconds for FFmpeg
     duration_seconds = duration_ms / 1000.0
-    
+
     # Generate silence using FFmpeg with matching properties
     ffmpeg_cmd = [
-        "ffmpeg", "-y", 
-        "-f", "lavfi", 
+        "ffmpeg", "-y",
+        "-f", "lavfi",
         "-i", f"anullsrc=r={sample_rate}:cl={channel_layout}",
         "-t", str(duration_seconds),
         "-c:a", "pcm_s16le",  # WAV format
         "-ar", str(sample_rate),  # Match sample rate
         output_path
     ]
-    
+
     try:
         # Use centralized secure command execution
         allowed_commands = ['ffmpeg']
         result = run_shell_command_secure(ffmpeg_cmd, allowed_commands)
-        
+
         if not result or result.returncode != 0:
             error_msg = result.stderr if result else "Unknown error"
             raise RuntimeError(f"Failed to generate silence file: {error_msg}")
-            
+
         print(f"Generated silence file: {output_path} ({duration_ms}ms, {sample_rate}Hz, {channel_layout})")
     except Exception as e:
         print(f"Error generating silence file: {e}")
-- 
2.36.1.windows.1

